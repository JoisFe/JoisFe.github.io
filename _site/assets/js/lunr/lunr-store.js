var store = [{
        "title": "Edge Case: Nested and Mixed Lists",
        "excerpt":"Nested and mixed lists are an interesting beast. It’s a corner case to make sure that      Lists within lists do not break the ordered list numbering order   Your list styles go deep enough.   Ordered – Unordered – Ordered      ordered item   ordered item            unordered       unordered                    ordered item           ordered item                           ordered item   ordered item   Ordered – Unordered – Unordered      ordered item   ordered item            unordered       unordered                    unordered item           unordered item                           ordered item   ordered item   Unordered – Ordered – Unordered      unordered item   unordered item            ordered       ordered                    unordered item           unordered item                           unordered item   unordered item   Unordered – Unordered – Ordered      unordered item   unordered item            unordered       unordered                    ordered item           ordered item                           unordered item   unordered item   Task Lists      Finish my changes   Push my commits to GitHub   Open a pull request  ","categories": ["Deep_Learning"],
        "tags": ["hew"],
        "url": "/deep_learning/hihi/",
        "teaser": null
      },{
        "title": "Xx",
        "excerpt":"from sklearn.datasets import load_diabetes #사이킷런에서 당뇨병 환자 데이터 가져옴 diabetes = load_diabetes()   print(diabetes.data.shape, diabetes.target.shape) # 입력 데이터와 타깃 데이터 크기   (442, 10) (442,)   diabetes.data[0:3] # 당뇨병 환자 데이터 앞부분 3개만   array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,         -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613],        [-0.00188202, -0.04464164, -0.05147406, -0.02632783, -0.00844872,         -0.01916334,  0.07441156, -0.03949338, -0.06832974, -0.09220405],        [ 0.08529891,  0.05068012,  0.04445121, -0.00567061, -0.04559945,         -0.03419447, -0.03235593, -0.00259226,  0.00286377, -0.02593034]])   diabetes.target[:3] # 당뇨병 환자 타깃 데이터 앞부분 3개만   array([151.,  75., 141.])   import matplotlib.pyplot as plt plt.scatter(diabetes.data[:, 2], diabetes.target) plt.xlabel('x') plt.ylabel('y') plt.show() #x축은 입력 데이터, y축은 타깃 데이터로 나타낸 그래프      #훈련 데이터 준비 x = diabetes.data[:, 2] #입력 데이터의 세번재 특성 분리하여 x에 저장 y = diabetes.target  #타깃 데이터를 y에 저장   w = 1.0 b = 1.0 # 션형 회귀에서의 가중치(기울기) = w, 절편 = b 를 둘다 1로 임의로 지정  # y = w * x + b 형태   y_hat = x[0]*w + b # 입력 데이터의 첫번째 샘플 x[0]에 대한 예측 데이터 y_hat을 구해본다. print(y_hat)   1.0616962065186886   print(y[0]) # 첫 번째 샘플 데이터 x[0]에 대응하는 타깃값 y[0]을 출력   151.0   # y[0]과 우리가 예측한 y_hat과 차이가 너무 많이남 -&gt; 우리가 만든 선형 방정식은 현재 데이터에 맞지 않다는 뜻  # w, b를 바꿔야 한다.!! 어떻게 ? y_hat이 y[0]에 비슷해 지도록   w_inc = w + 0.1 # w를 변화시킨 값 w_inc는 w에 0.1을 더한다 (0.1은 아무 의미 x 그냥 더해보는 것) y_hat_inc = x[0] * w_inc + b # 이번 선형 방정식에 w 대신 w_inc로 바꾼다.  print(y_hat_inc) # w_inc로 가중치를 바꾸면서 나오게 된 에측 결과를 y_hat_inc   1.0678658271705574   #이전 예측값 y_hat보다 y_hat_inc가 좀더 y[0]에 가까워 지긴 했다.   #w가 0.1 증가하면서 y_hat이 얼마나 변했는지 확인해 보면  w_rate = (y_hat_inc - y_hat) / (w_inc - w) # w_rate는 예측값의 증가 정도를 나타낸다.  print(w_rate) # 이 w_rate를 훈련 데이터 x[0]에 대한 w의 변화율 이라고 한다.   0.061696206518688734   print(x[0]) # 뭐지 ??? x[0]값과 w_rate(x[0]에 대한 w의 변화율)이 같은 값이다....  # w가 1만큼 증가한다고 가정해보자 그러면 y_hat은 x[0]만큼 증가 할 것 아닌가 # w 변화율 = w가 1만큼 증가할 때 y_hat이 얼만큼 증가하는지에 대한 의미와 같으니 # 당연히 선형 방정식에서는 w 변화율은 x값과 같을 것이다. # 식으로도 증명 가능  #w_rate == (y_hat_inc - y_hat) / (w_inc - w) == {(x[0] * w_inc + b) - (x[0] * w + b)} / (w_inc - w) # = {x[0] * ((w + 0.1) - w)} / ((w + 0.1) - w) = x[0]   0.0616962065186885   # 이제는 가중치(기울기)를 업데이트 하는 방법에 대해 알아보자. (y_hat과 y가 비슷해지게 가중치를 업데이트 하는 방법ㅂ)  w_new = w + w_rate # 기존 가중치에 가중치의 변화율을 더하여 가중치를 업데이트 한다. print(w_new)  #왜 이런 방법을 ? # 1. 변화율이 양수일 때 : y_hat이 y에 미치지 못하는 상황에서 변화율이 양수이면 w를 증가 시켜야 y_hat이 증가한다. # 이때 w의 변화율이 양수이므로 w에 w의 변화율을 더해주면 w는 증가하게 된다.  # 2. 변화율이 음수일 때 : y_hat이 y에 미치지 못하는 상황에서 변화율이 음수이면 w를 감소 시켜야 y_hat이 증가한다. # 이때 w의 변화율이 음수이므로 w에 w의 변화율을 더해주면 w는 감소하게 된다.  # 즉 변화율이 양수이든 음수이든 w에 w변화율을 더해주면 y_hat이 y에 가까워지는 방향으로 w가 변한다.   1.0616962065186888   # 이전에는 w(가중치)를 변화시켰다면 b(절편)을 변화시켜 보자. # 절편도 마찬가지 예측값이 타깃값에 가까워지도록 변화시키는 것이 목적!!  b_inc = b + 0.1 # 절편 b에 0.1을 더해보자 (0.1은 아무 의미 없음 그냥 더해보는 것) y_hat_inc = x[0] * w + b_inc print(y_hat_inc) # y_hat_inc가 y_hat 보다 y에 가까움   1.1616962065186887   b_rate = (y_hat_inc - y_hat) / (b_inc - b) print(b_rate) #이번에 b의 변화율에 대해 구해보면 1이 나온다 딱 1!!!!  #선형 방정식에서 b가 1만큼 증가하면 y_hat또한 당연히 1만큼 증가하겠지 #식으로도 증명가능 #b_rate = (y_hat_inc - y_hat) / (b_inc - b) ={} (x[0] * w + b_inc) - (x[0] * w + b)} / (b_inc - b) # = {(b + 0.1) - b} / {(b + 0.1) - b} = 1   1.0   # w와 마찬가지로 b(절편) 또한 업데이트를 하여 좀 더 타깃값과 유사한 예측값을 내놓게 해야한다. b_new = b + b_rate # b_rate = 1 # w 업데이트와 같은방법으로 b(절편) 또한 기존 b에 b_rate를 더하여 b를 갱신한다. (b_rate 대신 1로 해도 됨)   ''' 위의 방법으로 w, b를 업데이트 한다고 생각해보자 w, b를 변화시켜도 y_hat이 매우 조금 변한다. 이 방법으로는 y와 y_hat의 차이가 매우 큰 경우 y_hat을 y와 유사한 값으로 업데이트 시키는 시간이 매우 오래 걸릴 것 이다.  따라서 위으 문제점을 해결하는 방법으로 \"오차 역전파\"를 이용한다ㅏ.  오차 역전파 (backpropagation) '''   err = y[0] - y_hat # 에러 값을 구한다. 에러값 = 실제 값 - 예측값  ''' 오차 역전파 :  이전에 w, b를 갱신할때 w에는 w_rate를 더해주고, b에는 b_rate를 더해줬음 하지만 오차 역전파 방법은 w에 w_rate * 에러값, b에는 b_rate * 에러값 을 더해주는 방식으로 w와 b를 갱신한다.  왜 이런방법을 쓸까?  w_new = w + w_rate * err  b_new = b + b_rate * err  #  print(w_new, b_new)   911.1983904448475 156.2427820067777   y_hat = x[1] * w_new + b_new err = y[1] - y_hat w_rate = x[1] b_rate = 1  w_new = w_new + w_rate * err b_new = b_new + b_rate * err print(w_new, b_new)   14.132317616381767 75.52764127612664   for x_i, y_i in zip(x, y):   y_hat = x_i * w + b    err = y_i - y_hat    w_rate = x_i   w = w + w_rate * err    b_rate = 1   b = b + b_rate * err  print(w, b)   587.8654539985689 99.40935564531424   plt.scatter(x, y) pt1 = (-0.1, -0.1 * w + b) pt2 = (0.15, 0.15 * w + b)  plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]]) plt.xlabel('x') plt.ylabel('y') plt.show()      for i in range(0, 100):   for x_i, y_i in zip(x, y):     y_hat = x_i * w + b     err = y_i - y_hat      w_rate = x_i     w = w + w_rate * err      b_rate = 1     b = b + b_rate * err  print(w, b)   913.5973364345905 123.39414383177204   plt.scatter(x, y) pt1 = (-0.1, -0.1 * w + b) pt2 = (0.15, 0.15 * w + b) plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]]) plt.xlabel('x') plt.ylabel('y') plt.show()      x_new = 0.18 y_pred = x_new * w + b print(y_pred)   287.8416643899983   plt.scatter(x, y) plt.scatter(x_new, y_pred) plt.xlabel('x') plt.ylabel('y') plt.show()      class Neuron:   def __init__(self):     self.w = 1.0     self.b = 1.0    def forpass(self, x):     y_hat = x * self.w + self.b      return y_hat    def backprop(self, x, err):     w_rate = x     b_rate = 1      w_grad = w_rate * err     b_grad = b_rate * err      return w_grad, b_grad    def fit(self, x, y, epochs = 100):     for i in range(epochs):       for x_i, y_i in zip(x, y):         y_hat = self.forpass(x_i)                  err = -(y_i - y_hat)          w_grad, b_grad = self.backprop(x_i, err)          self.w -= w_grad         self.b -= b_grad  neuron = Neuron() neuron.fit(x, y)  plt.scatter(x, y) pt1 = (-0.1, -0.1 * neuron.w + neuron.b) pt2 = (0.15, 0.15 * neuron.w + neuron.b)  plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])  plt.xlabel('x') plt.ylabel('y')  plt.show()      import numpy as np test_array = np.array([1,2,3,4]) print(test_array)  test_array = test_array.reshape(2,2) print(test_array)   [1 2 3 4] [[1 2]  [3 4]]   ","categories": [],
        "tags": [],
        "url": "/xx/",
        "teaser": null
      },{
        "title": "Xyz",
        "excerpt":"[HW1.2] LR vs MLP     Import packages   Preprocess Dataset   Pytorch Dataset   Model   Trainer   PyTorch에서 모델을 학습시키는 과정은 크게 3단계로 나누어져 진행됩니다.     Initilaize Dataset and DataLoader   Initialize Model   Train Model   이번 실습에서는 머신러닝 수업때 활용했던 Mnist dataset을 활용하여 logistic regression model과 MLP model을 구현해보고 학습 파이프라인을 익혀보도록 하겠습니다.   1. Import packages      필요한 package를 설치하고 import합니다    런타임의 유형을 변경해줍니다.   상단 메뉴에서 [런타임]-&gt;[런타임유형변경]-&gt;[하드웨어가속기]-&gt;[GPU]   변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.   import torch import torch.nn as nn print(torch.__version__) print(torch.cuda.is_available())   1.9.0+cu102 True   import matplotlib.pyplot as plt import numpy as np import scipy as sp from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split  np.set_printoptions(precision=3) np.set_printoptions(suppress=True)   2. Preprocess Dataset   Load Dataset   mnist = fetch_openml('mnist_784', cache=False)   mnist.data.shape   (70000, 784)   Preprocess Dataset   Mnist에 존재하는 각각의 image는 28*28의 픽셀로 구성된 784차원 짜리 벡터로 나타나져 있습니다. 각 픽셀은 0-255사이의 값으로 흰색부터 검은색 사이의 값을 나타냅니다.   X = mnist.data.astype('float32') y = mnist.target.astype('int64') print(X.shape) print(y.shape)   (70000, 784) (70000,)   값이 너무 커지는 것을 방지하기 위해 [0,255]사이의 input을 [0,1]의 scale로 조정해줍니다.   X /= 255.0   print(X.min(), X.max())   0.0 1.0   Split Dataset   학습과 평가를 위한 dataset으로 나눕니다.   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5) print(X_train.shape) print(y_train.shape) print(X_val.shape) print(y_val.shape) print(X_test.shape) print(y_test.shape)   (56000, 784) (56000,) (7000, 784) (7000,) (7000, 784) (7000,)   Visualize Dataset   def plot_example(X, y):     \"\"\"Plot the first 5 images and their labels in a row.\"\"\"     for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):         plt.subplot(151 + i)         plt.imshow(img)         plt.xticks([])         plt.yticks([])         plt.title(y)   plot_example(X_train, y_train)      3. Pytorch Dataset   PyTorch에서는 Custom Dataset을 사용하기 위해서는 torch.utils.data.Dataset의 형태로 dataset class를 정의해준 이후, torch.utils.data.DataLoader의 형태로 dataloader class를 정의하여 학습시에 model에 forwarding할 data를 sample해줍니다.   (https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)   Dataset   가장 보편적으로 사용되는 map-style의 dataset class는 torch.utils.data.Dataset을 superclass로 받아 getitem()과 len()함수를 override해줍니다.   class CustomDataset(torch.utils.data.Dataset):   def __init__(self, data):     self.data = data      def __getitem__(self, index):     return self.data[index]    def __len__(self):     return len(self.data)   class CustomDataset(torch.utils.data.Dataset):     def __init__(self, X, y):         super(CustomDataset, self).__init__()         self.X = X         self.y = y              def __getitem__(self, index):         x = self.X[index]         y = self.y[index]         x = torch.from_numpy(x).float()         y = torch.from_numpy(np.array(y)).long()         return x, y      def __len__(self):         return len(self.X)   train_dataset = CustomDataset(X_train, y_train) val_dataset = CustomDataset(X_val, y_val) test_dataset = CustomDataset(X_test, y_test)  print(len(train_dataset)) print(train_dataset.X.shape) print(len(val_dataset)) print(val_dataset.X.shape) print(len(test_dataset)) print(test_dataset.X.shape)   56000 (56000, 784) 7000 (7000, 784) 7000 (7000, 784)   DataLoader   DataLoader는 train 혹은 validation시 dataset에서 batch를 sampling하기 위한 API입니다 (https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).   필수적으로 사용하는 option들은 아래와 같습니다.     dataset: sampling할 dataset   batch_size: 한번에 sampling할 dataset의 개수   shuffle: 1 epoch를 기준으로 dataset을 shuffle할지   더 자세한 option이 궁금하시다면 api를 참고해주세요.   batch_size = 64  # shuffle the train data train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # do not shuffle the val &amp; test data val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False) test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # dataset size // batch_size print(len(train_dataloader)) print(len(val_dataloader)) print(len(test_dataloader))   875 110 110   4. Model   Pytorch에서 model을 선언할 때는 torch.nn.Module class를 superclass로 받아 init()함수와 forward() 함수를 작성해줍니다.   init()함수에는 모델의 파라미터들을 선언하고, forward함수에는 해당 파라미터들을 이용하여 data를 model에 통과시켜줍니다.   https://pytorch.org/docs/stable/generated/torch.nn.Module.html   Initialize Logistic Regression Model   class LR(nn.Module):     def __init__(self, input_dim, output_dim):         super(LR, self).__init__()         self.fc = nn.Linear(input_dim, output_dim)      def forward(self, x):         x = self.fc(x)         return x   Initialize MLP Model   class MLP(torch.nn.Module):     def __init__(self, input_dim, hidden_dim, output_dim):         super(MLP, self).__init__()         self.fc1 = nn.Linear(input_dim, hidden_dim)         self.relu = nn.ReLU()         self.fc2 = nn.Linear(hidden_dim, output_dim)      def forward(self, x):         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         return x   5. Train   이제 선언한 model을 통해 학습을 진행하기 위해서는 model의 파라미터를 최적화할 optimizer가 필요합니다. 이번 실습에서는 가장 보편적으로 사용되는 Adam optimizer를 사용하겠습니다.   Trainer   class Trainer():     def __init__(self, trainloader, valloader, testloader, model, optimizer, criterion, device):         \"\"\"         trainloader: train data's loader         testloader: test data's loader         model: model to train         optimizer: optimizer to update your model         criterion: loss function         \"\"\"         self.trainloader = trainloader         self.valloader = valloader         self.testloader = testloader         self.model = model         self.optimizer = optimizer         self.criterion = criterion         self.device = device              def train(self, epoch = 1):         # 학습을 시작할 때 model을 train-mode로 바꿔주어야함.         # Question) 왜 model을 train-mode로 바꿔야 할까요?           # train-mode로 바꾸면 convolution 또는 Linear 뿐만 아니라 dropOut과 Batch Normalization과 같이 parameter를 가진 Layer들도 학습하기 위해 준비하게 한다.         self.model.train()         for e in range(epoch):             running_loss = 0.0               for i, data in enumerate(self.trainloader, 0):                  inputs, labels = data                  # model에 input으로 tensor를 gpu-device로 보낸다                 inputs = inputs.to(self.device)                   labels = labels.to(self.device)                 # zero the parameter gradients                 # Question) 왜 optimizer.zero_grad()라는 함수를 사용해야 할까요?                 # pytorch에서는 gradient값을 backward를 해줄때 게속 더해주기 때문에 0으로 만든다.                 self.optimizer.zero_grad()                     # forward + backward + optimize                 # get output after passing through the network                 outputs = self.model(inputs)                  # compute model's score using the loss function                 loss = self.criterion(outputs, labels)                   # perform back-propagation from the loss                 loss.backward()                  # gradient descent를 통해 model의 output을 얻는다.                 self.optimizer.step()                  running_loss += loss.item()                          print('epoch: %d  loss: %.3f' % (e + 1, running_loss / len(self.trainloader)))             running_loss = 0.0         val_acc = self.validate()         return val_acc      def validate(self):         # Question) 왜 model을 eval-mode로 바꿔야 할까요?         # eval-mode로 바꾸면 Batch Normalization이나 Drop Out 같은 Layer들을 잠금         self.model.eval()          correct = 0         for inputs, labels in self.valloader:             inputs = inputs.to(self.device)             labels = labels.to(self.device)             output = self.model(inputs)              pred = output.max(1, keepdim=True)[1] # get the index of the max              correct += pred.eq(labels.view_as(pred)).sum().item()         return correct / len(self.valloader.dataset)              def test(self):         self.model.eval()          correct = 0         for inputs, labels in self.testloader:             inputs = inputs.to(self.device)             labels = labels.to(self.device)             output = self.model(inputs)              pred = output.max(1, keepdim=True)[1] # get the index of the max              correct += pred.eq(labels.view_as(pred)).sum().item()         return correct / len(self.testloader.dataset)    Logistic Regression   input_dim = 784 output_dim = 10 epoch = 4 device = torch.device('cuda')  best_acc = 0.0 lrs = [1e-1, 1e-2, 1e-3, 1e-4] for lr in lrs:     model = LR(input_dim=input_dim, output_dim=output_dim).to(device)     criterion = nn.CrossEntropyLoss().to(device)     optimizer = torch.optim.Adam(model.parameters(), lr=lr)     trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)     val_acc = trainer.train(epoch = epoch)     print('val_acc: %.3f' %(val_acc))     if val_acc &gt; best_acc:         best_acc = val_acc         torch.save(model.state_dict(), './best_model')  trainer.model.load_state_dict(torch.load('./best_model')) test_acc = trainer.test() print('test_acc: %.3f' %(test_acc))   epoch: 1  loss: 1.143 epoch: 2  loss: 1.226 epoch: 3  loss: 1.248 epoch: 4  loss: 1.254 val_acc: 0.886 epoch: 1  loss: 0.356 epoch: 2  loss: 0.303 epoch: 3  loss: 0.296 epoch: 4  loss: 0.290 val_acc: 0.912 epoch: 1  loss: 0.550 epoch: 2  loss: 0.323 epoch: 3  loss: 0.294 epoch: 4  loss: 0.280 val_acc: 0.925 epoch: 1  loss: 1.391 epoch: 2  loss: 0.734 epoch: 3  loss: 0.556 epoch: 4  loss: 0.473 val_acc: 0.891 test_acc: 0.916   MLP   input_dim = 784 hidden_dim = 32 output_dim = 10 epoch = 4 device = torch.device('cuda')  best_acc = 0.0 lrs = [1e-1, 1e-2, 1e-3, 1e-4] for lr in lrs:     model = MLP(input_dim=input_dim,                  hidden_dim=hidden_dim,                 output_dim=output_dim).to(device)     criterion = nn.CrossEntropyLoss().to(device)     optimizer = torch.optim.Adam(model.parameters(), lr=lr)     trainer = Trainer(train_dataloader, val_dataloader, test_dataloader, model, optimizer, criterion, device)     val_acc = trainer.train(epoch = epoch)     print('val_acc: %.3f' %(val_acc))     if val_acc &gt; best_acc:         best_acc = val_acc         torch.save(model.state_dict(), './best_model')  trainer.model.load_state_dict(torch.load('./best_model')) test_acc = trainer.test() print('test_acc: %.3f' %(test_acc))   epoch: 1  loss: 1.093 epoch: 2  loss: 1.100 epoch: 3  loss: 1.185 epoch: 4  loss: 1.193 val_acc: 0.491 epoch: 1  loss: 0.282 epoch: 2  loss: 0.170 epoch: 3  loss: 0.145 epoch: 4  loss: 0.129 val_acc: 0.954 epoch: 1  loss: 0.465 epoch: 2  loss: 0.240 epoch: 3  loss: 0.198 epoch: 4  loss: 0.171 val_acc: 0.952 epoch: 1  loss: 1.304 epoch: 2  loss: 0.553 epoch: 3  loss: 0.409 epoch: 4  loss: 0.354 val_acc: 0.911 test_acc: 0.948     ","categories": [],
        "tags": [],
        "url": "/xyz/",
        "teaser": null
      },{
        "title": "수치 예측",
        "excerpt":"from sklearn.datasets import load_diabetes #사이킷런에서 당뇨병 환자 데이터 가져옴 diabetes = load_diabetes()   print(diabetes.data.shape, diabetes.target.shape) # 입력 데이터와 타깃 데이터 크기   (442, 10) (442,)   diabetes.data[0:3] # 당뇨병 환자 데이터 앞부분 3개만   array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,         -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613],        [-0.00188202, -0.04464164, -0.05147406, -0.02632783, -0.00844872,         -0.01916334,  0.07441156, -0.03949338, -0.06832974, -0.09220405],        [ 0.08529891,  0.05068012,  0.04445121, -0.00567061, -0.04559945,         -0.03419447, -0.03235593, -0.00259226,  0.00286377, -0.02593034]])   diabetes.target[:3] # 당뇨병 환자 타깃 데이터 앞부분 3개만   array([151.,  75., 141.])   import matplotlib.pyplot as plt plt.scatter(diabetes.data[:, 2], diabetes.target) plt.xlabel('x') plt.ylabel('y') plt.show() #x축은 입력 데이터, y축은 타깃 데이터로 나타낸 그래프      #훈련 데이터 준비 x = diabetes.data[:, 2] #입력 데이터의 세번재 특성 분리하여 x에 저장 y = diabetes.target  #타깃 데이터를 y에 저장   w = 1.0 b = 1.0 # 션형 회귀에서의 가중치(기울기) = w, 절편 = b 를 둘다 1로 임의로 지정  # y = w * x + b 형태   y_hat = x[0]*w + b # 입력 데이터의 첫번째 샘플 x[0]에 대한 예측 데이터 y_hat을 구해본다. print(y_hat)   1.0616962065186886   print(y[0]) # 첫 번째 샘플 데이터 x[0]에 대응하는 타깃값 y[0]을 출력   151.0   # y[0]과 우리가 예측한 y_hat과 차이가 너무 많이남 -&gt; 우리가 만든 선형 방정식은 현재 데이터에 맞지 않다는 뜻  # w, b를 바꿔야 한다.!! 어떻게 ? y_hat이 y[0]에 비슷해 지도록   w_inc = w + 0.1 # w를 변화시킨 값 w_inc는 w에 0.1을 더한다 (0.1은 아무 의미 x 그냥 더해보는 것) y_hat_inc = x[0] * w_inc + b # 이번 선형 방정식에 w 대신 w_inc로 바꾼다.  print(y_hat_inc) # w_inc로 가중치를 바꾸면서 나오게 된 에측 결과를 y_hat_inc   1.0678658271705574   #이전 예측값 y_hat보다 y_hat_inc가 좀더 y[0]에 가까워 지긴 했다.   #w가 0.1 증가하면서 y_hat이 얼마나 변했는지 확인해 보면  w_rate = (y_hat_inc - y_hat) / (w_inc - w) # w_rate는 예측값의 증가 정도를 나타낸다.  print(w_rate) # 이 w_rate를 훈련 데이터 x[0]에 대한 w의 변화율 이라고 한다.   0.061696206518688734   print(x[0]) # 뭐지 ??? x[0]값과 w_rate(x[0]에 대한 w의 변화율)이 같은 값이다....  # w가 1만큼 증가한다고 가정해보자 그러면 y_hat은 x[0]만큼 증가 할 것 아닌가 # w 변화율 = w가 1만큼 증가할 때 y_hat이 얼만큼 증가하는지에 대한 의미와 같으니 # 당연히 선형 방정식에서는 w 변화율은 x값과 같을 것이다. # 식으로도 증명 가능  #w_rate == (y_hat_inc - y_hat) / (w_inc - w) == {(x[0] * w_inc + b) - (x[0] * w + b)} / (w_inc - w) # = {x[0] * ((w + 0.1) - w)} / ((w + 0.1) - w) = x[0]   0.0616962065186885   # 이제는 가중치(기울기)를 업데이트 하는 방법에 대해 알아보자. (y_hat과 y가 비슷해지게 가중치를 업데이트 하는 방법ㅂ)  w_new = w + w_rate # 기존 가중치에 가중치의 변화율을 더하여 가중치를 업데이트 한다. print(w_new)  #왜 이런 방법을 ? # 1. 변화율이 양수일 때 : y_hat이 y에 미치지 못하는 상황에서 변화율이 양수이면 w를 증가 시켜야 y_hat이 증가한다. # 이때 w의 변화율이 양수이므로 w에 w의 변화율을 더해주면 w는 증가하게 된다.  # 2. 변화율이 음수일 때 : y_hat이 y에 미치지 못하는 상황에서 변화율이 음수이면 w를 감소 시켜야 y_hat이 증가한다. # 이때 w의 변화율이 음수이므로 w에 w의 변화율을 더해주면 w는 감소하게 된다.  # 즉 변화율이 양수이든 음수이든 w에 w변화율을 더해주면 y_hat이 y에 가까워지는 방향으로 w가 변한다.   1.0616962065186888   # 이전에는 w(가중치)를 변화시켰다면 b(절편)을 변화시켜 보자. # 절편도 마찬가지 예측값이 타깃값에 가까워지도록 변화시키는 것이 목적!!  b_inc = b + 0.1 # 절편 b에 0.1을 더해보자 (0.1은 아무 의미 없음 그냥 더해보는 것) y_hat_inc = x[0] * w + b_inc print(y_hat_inc) # y_hat_inc가 y_hat 보다 y에 가까움   1.1616962065186887   b_rate = (y_hat_inc - y_hat) / (b_inc - b) print(b_rate) #이번에 b의 변화율에 대해 구해보면 1이 나온다 딱 1!!!!  #선형 방정식에서 b가 1만큼 증가하면 y_hat또한 당연히 1만큼 증가하겠지 #식으로도 증명가능 #b_rate = (y_hat_inc - y_hat) / (b_inc - b) ={} (x[0] * w + b_inc) - (x[0] * w + b)} / (b_inc - b) # = {(b + 0.1) - b} / {(b + 0.1) - b} = 1   1.0   # w와 마찬가지로 b(절편) 또한 업데이트를 하여 좀 더 타깃값과 유사한 예측값을 내놓게 해야한다. b_new = b + b_rate # b_rate = 1 # w 업데이트와 같은방법으로 b(절편) 또한 기존 b에 b_rate를 더하여 b를 갱신한다. (b_rate 대신 1로 해도 됨)   ''' 위의 방법으로 w, b를 업데이트 한다고 생각해보자 w, b를 변화시켜도 y_hat이 매우 조금 변한다. 이 방법으로는 y와 y_hat의 차이가 매우 큰 경우 y_hat을 y와 유사한 값으로 업데이트 시키는 시간이 매우 오래 걸릴 것 이다.  따라서 위으 문제점을 해결하는 방법으로 \"오차 역전파\"를 이용한다ㅏ.  오차 역전파 (backpropagation) '''   err = y[0] - y_hat # 에러 값을 구한다. 에러값 = 실제 값 - 예측값  ''' 오차 역전파 :  이전에 w, b를 갱신할때 w에는 w_rate를 더해주고, b에는 b_rate를 더해줬음 하지만 오차 역전파 방법은 w에 w_rate * 에러값, b에는 b_rate * 에러값 을 더해주는 방식으로 w와 b를 갱신한다.  왜 이런방법을 쓸까?  w_new = w + w_rate * err  b_new = b + b_rate * err  #  print(w_new, b_new)   911.1983904448475 156.2427820067777   y_hat = x[1] * w_new + b_new err = y[1] - y_hat w_rate = x[1] b_rate = 1  w_new = w_new + w_rate * err b_new = b_new + b_rate * err print(w_new, b_new)   14.132317616381767 75.52764127612664   for x_i, y_i in zip(x, y):   y_hat = x_i * w + b    err = y_i - y_hat    w_rate = x_i   w = w + w_rate * err    b_rate = 1   b = b + b_rate * err  print(w, b)   587.8654539985689 99.40935564531424   plt.scatter(x, y) pt1 = (-0.1, -0.1 * w + b) pt2 = (0.15, 0.15 * w + b)  plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]]) plt.xlabel('x') plt.ylabel('y') plt.show()      for i in range(0, 100):   for x_i, y_i in zip(x, y):     y_hat = x_i * w + b     err = y_i - y_hat      w_rate = x_i     w = w + w_rate * err      b_rate = 1     b = b + b_rate * err  print(w, b)   913.5973364345905 123.39414383177204   plt.scatter(x, y) pt1 = (-0.1, -0.1 * w + b) pt2 = (0.15, 0.15 * w + b) plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]]) plt.xlabel('x') plt.ylabel('y') plt.show()      x_new = 0.18 y_pred = x_new * w + b print(y_pred)   287.8416643899983   plt.scatter(x, y) plt.scatter(x_new, y_pred) plt.xlabel('x') plt.ylabel('y') plt.show()      class Neuron:   def __init__(self):     self.w = 1.0     self.b = 1.0    def forpass(self, x):     y_hat = x * self.w + self.b      return y_hat    def backprop(self, x, err):     w_rate = x     b_rate = 1      w_grad = w_rate * err     b_grad = b_rate * err      return w_grad, b_grad    def fit(self, x, y, epochs = 100):     for i in range(epochs):       for x_i, y_i in zip(x, y):         y_hat = self.forpass(x_i)                  err = -(y_i - y_hat)          w_grad, b_grad = self.backprop(x_i, err)          self.w -= w_grad         self.b -= b_grad  neuron = Neuron() neuron.fit(x, y)  plt.scatter(x, y) pt1 = (-0.1, -0.1 * neuron.w + neuron.b) pt2 = (0.15, 0.15 * neuron.w + neuron.b)  plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])  plt.xlabel('x') plt.ylabel('y')  plt.show()      import numpy as np test_array = np.array([1,2,3,4]) print(test_array)  test_array = test_array.reshape(2,2) print(test_array)   [1 2 3 4] [[1 2]  [3 4]]   ","categories": ["Machine_Learning"],
        "tags": ["선형 회귀","경사 하강법","손실 함수"],
        "url": "/machine_learning/%EC%88%98%EC%B9%98-%EC%98%88%EC%B8%A1/",
        "teaser": null
      }]
