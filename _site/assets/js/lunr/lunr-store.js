var store = [{
        "title": "수치 예측",
        "excerpt":"가장 먼저 딥러닝의 기초가 되는 머신러닝 알고리즘 중 가장 간단한 선형회귀를 만들어 볼 것이다.    직선의 그래프 y = ax + b  a : 기울기, b : 절편   선형 회귀(Linear Regression) :    위 선형 방정식의 기울기 a와 절편 b를 찾아내는 방법 즉 선형 방정식(선형 함수)를 예측하는 것  -&gt; 입력 데이터 x와 타깃 데이터 y를 통해 기울기 a와 절편 b를 찾는 것      경사 하강법(Gradient descent) :    위 선형회귀의 목표인 입력 데이터와 타깃 데이터가 주어질 때 그 관계를 잘 표현하는 직선의 방정식(선형 방정식)을 찾아내는 방법   -&gt; 기울기 a, 절편 b를 찾으면 직선의 방정식을 찾을 수 있겠지…  설명이 조금 애매함… 차후 좀더 구체적으로 설명할 것임    경사 하강법은 구체적으로 모델이 데이터를 잘 표현할 수 있도록 기울기(변화율)을 이용하여 모델을 조절하는 방식   -&gt; 즉 기울기를 이용한다는 점!!!! 무슨말인지 이해가 안되면 아래 예제 코드를 이용해 알아보면 된다.      선형 회귀를 푸는 알고리즘은 매우 많은 그 중의 하나가 경사 하강법임!!   from sklearn.datasets import load_diabetes #사이킷런에서 당뇨병 환자 데이터 가져옴 diabetes = load_diabetes()   print(diabetes.data.shape, diabetes.target.shape) # 입력 데이터와 타깃 데이터 크기   (442, 10) (442,)   diabetes.data[0:3] # 당뇨병 환자 데이터 앞부분 3개만   array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,         -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613],        [-0.00188202, -0.04464164, -0.05147406, -0.02632783, -0.00844872,         -0.01916334,  0.07441156, -0.03949338, -0.06832974, -0.09220405],        [ 0.08529891,  0.05068012,  0.04445121, -0.00567061, -0.04559945,         -0.03419447, -0.03235593, -0.00259226,  0.00286377, -0.02593034]])   diabetes.target[:3] # 당뇨병 환자 타깃 데이터 앞부분 3개만   array([151.,  75., 141.])   import matplotlib.pyplot as plt plt.scatter(diabetes.data[:, 2], diabetes.target) plt.xlabel('x') plt.ylabel('y') plt.show() #x축은 입력 데이터, y축은 타깃 데이터로 나타낸 그래프      #훈련 데이터 준비 x = diabetes.data[:, 2] #입력 데이터의 세번재 특성 분리하여 x에 저장 y = diabetes.target  #타깃 데이터를 y에 저장   w = 1.0 b = 1.0 # 션형 회귀에서의 가중치(기울기) = w, 절편 = b 를 둘다 1로 임의로 지정  # y = w * x + b 형태   y_hat = x[0]*w + b # 입력 데이터의 첫번째 샘플 x[0]에 대한 예측 데이터 y_hat을 구해본다. print(y_hat)   1.0616962065186886   print(y[0]) # 첫 번째 샘플 데이터 x[0]에 대응하는 타깃값 y[0]을 출력   151.0   # y[0]과 우리가 예측한 y_hat과 차이가 너무 많이남 -&gt; 우리가 만든 선형 방정식은 현재 데이터에 맞지 않다는 뜻  # w, b를 바꿔야 한다.!! 어떻게 ? y_hat이 y[0]에 비슷해 지도록   w_inc = w + 0.1 # w를 변화시킨 값 w_inc는 w에 0.1을 더한다 (0.1은 아무 의미 x 그냥 더해보는 것) y_hat_inc = x[0] * w_inc + b # 이번 선형 방정식에 w 대신 w_inc로 바꾼다.  print(y_hat_inc) # w_inc로 가중치를 바꾸면서 나오게 된 에측 결과를 y_hat_inc   1.0678658271705574   #이전 예측값 y_hat보다 y_hat_inc가 좀더 y[0]에 가까워 지긴 했다.   #w가 0.1 증가하면서 y_hat이 얼마나 변했는지 확인해 보면  w_rate = (y_hat_inc - y_hat) / (w_inc - w) # w_rate는 예측값의 증가 정도를 나타낸다.  print(w_rate) # 이 w_rate를 훈련 데이터 x[0]에 대한 w의 변화율 이라고 한다.   0.061696206518688734   print(x[0]) # 뭐지 ??? x[0]값과 w_rate(x[0]에 대한 w의 변화율)이 같은 값이다....  # w가 1만큼 증가한다고 가정해보자 그러면 y_hat은 x[0]만큼 증가 할 것 아닌가 # w 변화율 = w가 1만큼 증가할 때 y_hat이 얼만큼 증가하는지에 대한 의미와 같으니 # 당연히 선형 방정식에서는 w 변화율은 x값과 같을 것이다. # 식으로도 증명 가능  #w_rate == (y_hat_inc - y_hat) / (w_inc - w) == {(x[0] * w_inc + b) - (x[0] * w + b)} / (w_inc - w) # = {x[0] * ((w + 0.1) - w)} / ((w + 0.1) - w) = x[0]   0.0616962065186885   # 이제는 가중치(기울기)를 업데이트 하는 방법에 대해 알아보자. (y_hat과 y가 비슷해지게 가중치를 업데이트 하는 방법ㅂ)  w_new = w + w_rate # 기존 가중치에 가중치의 변화율을 더하여 가중치를 업데이트 한다. print(w_new)  #왜 이런 방법을 ? # 1. 변화율이 양수일 때 : y_hat이 y에 미치지 못하는 상황에서 변화율이 양수이면 w를 증가 시켜야 y_hat이 증가한다. # 이때 w의 변화율이 양수이므로 w에 w의 변화율을 더해주면 w는 증가하게 된다.  # 2. 변화율이 음수일 때 : y_hat이 y에 미치지 못하는 상황에서 변화율이 음수이면 w를 감소 시켜야 y_hat이 증가한다. # 이때 w의 변화율이 음수이므로 w에 w의 변화율을 더해주면 w는 감소하게 된다.  # 즉 변화율이 양수이든 음수이든 w에 w변화율을 더해주면 y_hat이 y에 가까워지는 방향으로 w가 변한다.   1.0616962065186888   # 이전에는 w(가중치)를 변화시켰다면 b(절편)을 변화시켜 보자. # 절편도 마찬가지 예측값이 타깃값에 가까워지도록 변화시키는 것이 목적!!  b_inc = b + 0.1 # 절편 b에 0.1을 더해보자 (0.1은 아무 의미 없음 그냥 더해보는 것) y_hat_inc = x[0] * w + b_inc print(y_hat_inc) # y_hat_inc가 y_hat 보다 y에 가까움   1.1616962065186887   b_rate = (y_hat_inc - y_hat) / (b_inc - b) print(b_rate) #이번에 b의 변화율에 대해 구해보면 1이 나온다 딱 1!!!!  #선형 방정식에서 b가 1만큼 증가하면 y_hat또한 당연히 1만큼 증가하겠지 #식으로도 증명가능 #b_rate = (y_hat_inc - y_hat) / (b_inc - b) ={} (x[0] * w + b_inc) - (x[0] * w + b)} / (b_inc - b) # = {(b + 0.1) - b} / {(b + 0.1) - b} = 1   1.0   # w와 마찬가지로 b(절편) 또한 업데이트를 하여 좀 더 타깃값과 유사한 예측값을 내놓게 해야한다. b_new = b + b_rate # b_rate = 1 # w 업데이트와 같은방법으로 b(절편) 또한 기존 b에 b_rate를 더하여 b를 갱신한다. (b_rate 대신 1로 해도 됨)   ''' 위의 방법으로 w, b를 업데이트 한다고 생각해보자 w, b를 변화시켜도 y_hat이 매우 조금 변한다. 이 방법으로는 y와 y_hat의 차이가 매우 큰 경우 y_hat을 y와 유사한 값으로 업데이트 시키는 시간이 매우 오래 걸릴 것 이다.  따라서 위으 문제점을 해결하는 방법으로 \"오차 역전파\"를 이용한다.  오차 역전파 (backpropagation) '''   err = y[0] - y_hat # 에러 값을 구한다. 에러값(오차) = 실제 값 - 예측값  ''' 오차 역전파 :  이전에 w, b를 갱신할때 w에는 w_rate를 더해주고, b에는 b_rate를 더해줬음 하지만 오차 역전파 방법은 y와 y_hat의 차이를 이용하여 w와 b를 업데이트 한다. -&gt; 오차가 연이어 전파되는 모습으로 수행  w에 w_rate * 에러값, b에는 b_rate * 에러값 을 더해주는 방식으로 w와 b를 갱신한다. '''  w_new = w + w_rate * err # 일단 처음에 x[0]일때 w의 변화율과 b의 변화율에 오차를 곱하여 w변화율과 b변화율을 갱신 b_new = b + b_rate * err    print(w_new, b_new) # -&gt; 이전에 오차를 이용하지 않은 방법에 비해 w변화율과 b변화율이 매우 크게 변했음   911.1983904448475 156.2427820067777   y_hat = x[1] * w_new + b_new # 위 방식대로 x[1]에 대해 err = y[1] - y_hat w_rate = x[1] b_rate = 1  w_new = w_new + w_rate * err b_new = b_new + b_rate * err print(w_new, b_new)   14.132317616381767 75.52764127612664   # x[0], x[1]을 이용해 w, b를 갱신하였다. 하지만 모든 입력 샘플에 대해 적용하여 w, b를 갱신하자 # 많이 할수록 w, b가 실제 데이터 타겟에 맞게 갱신되지 않을까? for x_i, y_i in zip(x, y):   y_hat = x_i * w + b    err = y_i - y_hat    w_rate = x_i   w = w + w_rate * err    b_rate = 1   b = b + b_rate * err  print(w, b)   587.8654539985689 99.40935564531424   ''' 위 방식으로 갱신한 w, b를 이용해 선형함수가 과연 실제 입력 x에 대한 타깃 y를 잘 예측할지  그래프에 나타내 보자.  산점도 -&gt; 입력 x값에 대한 실제 타깃 y값  직선 -&gt; 우리가 구해본 선형 함수(y = w * x + b)  ''' plt.scatter(x, y) pt1 = (-0.1, -0.1 * w + b) pt2 = (0.15, 0.15 * w + b)  plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]]) plt.xlabel('x') plt.ylabel('y') plt.show()  #먼가 아쉬움.... 직선이 실제 입력과 타깃과의 관계를 정확히 나타내지는 못하는 것 같음..      ''' 어떻게 하면 좀더 입력에 대한 실제 타깃과의 관계를 정확히 타나내는 선형 방정식을 구할 수 있을까/ 즉 더 적합한 w, b를 구할 수 있을까?  보통 경사 하강법은 주어진 trainning data로 학습을 여러 번 반복함  이전 까지는 한번만 반복햇음... -&gt; 적합한 w, b를 찾는데 학습량이 부족했음..  에포크(epoch) : 이렇게 전체 훈련데이터를 모두 이용하여 한 단위 작업을 진행하는 것 ''' for i in range(0, 100): #100번의 에포크 진행해 보자!!!   for x_i, y_i in zip(x, y):     y_hat = x_i * w + b     err = y_i - y_hat      w_rate = x_i     w = w + w_rate * err      b_rate = 1     b = b + b_rate * err  print(w, b) #100번의 에포크를 진행하고 난 뒤의 갱신된 w, b가 이전과 달라져 있음   913.5973364345905 123.39414383177204   #에포크 100번 진행한 뒤의 갱신된 w, b가 실제 x와 타깃 y의 관계를 잘 나타내는 선형방정식을 보이는지 #그래프를 통해 보자.  plt.scatter(x, y)  pt1 = (-0.1, -0.1 * w + b) pt2 = (0.15, 0.15 * w + b) plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]]) plt.xlabel('x') plt.ylabel('y') plt.show()  #에포크 1번 보다 훨씬 실제 데이터의 입력과 타깃 관계를 잘 나타내는 선형 방정식을 보인다. #즉 에포크를 늘리니 더 적합한 w, b를 갱신할 수 있다는 것을 알게 되었다.      x_new = 0.18 #그러면 이전에 구한 갱신된 w, b로 구한 선형방정식으로 새로운 입력 x에 대한 타깃 y를 예측해 보자. y_pred = x_new * w + b print(y_pred)   287.8416643899983   plt.scatter(x, y)  plt.scatter(x_new, y_pred) #새로운 입력 x에 대해 갱신된 w, b로 구한 선형방정식으로 예측한 y를 그래프에 나타내 보자. plt.xlabel('x') plt.ylabel('y') plt.show()  # 주황색? 으로 나타난 점이 새로운 입력 x에 대한 예측 y 값이다.. 실제 데이터 산점도에 어색하지 않은 위치인 것을 보니 괜찮게 예측한 것 같다..      이전에 정의한 경사 하강법은 조금 애매한 설명…. 다시 설명하자면   경사 하강법(Gradient descent) :    손실 함수가 정의되었을 때 손실 함수의 값이 최소가 되는 지점 찾는 방법       손실함수가 뭔데???…   손실 함수(loss function) = 비용 함수(cost function) = 목적 함수(objective function):    예상한 값과 실제 타깃값의 차이를 함수로 정의한 것         앞에서 적합한 가중치와 절편을 찾기 위해 사용한 경사 하강법에서 쓰인 손실 함수는 무엇일까?  바로 제곱 오차라는 손실 함수이다.   제곱 오차(Squared error) : SE = (y-y_hat)$^2$   타깃 값에서 예측 값을 뺀 후 제곱한 값      앞에서는 제곱 오차라는 손실 함수를 미분하여 가중치와 절편을 갱신하엿음..    미분????   이 부분을 알기 위해 제곱 오차라는 손실 함수를 파해쳐 보자       위에서 경사하강법이 곧 손실 함수의 값이 최소가 되는 지점을 찾는 방법 이라고 하였다.  그러면 최솟값을 어떻게 찾지???    아니 아까 예제 코드에서 w(가중치)와 b(절편)값 조절 하면서 y_hat 구했잖아….  y_hat과 y의 차이 즉 예측값과 실제 타깃 값 차이가 적으면 제곱 오차 함수 값도 작겠네 y- y_hat의 제곱이 작아질 테니…      아무튼 우리 w, b 갱신할 때 w_rate, b_rate 구하던거 기억날 것임.. 즉 가중치 변화량과 절편 변화량을 이용하여 w, b 갱신했잖아..    이 방법과 유사한 방식으로 w_rate(가중치 변화량), b_rate(절편 변화량) 구하는 것과 유사하게 가중치에 대한 손실 함수의 변화율과 절편에 대한 손실 함수의 변화율로 가중치와 절편을 갱신할 것이다.   구체적인 방법은 아래에서 설명   가중치에 대한 손실 함수에 (여기선 제곱 오차 함수) 변화량을 구하기 위해서 손실 함수에 가중치에 대해 미분을, 절편에 대한 손실 함수의 변화량 구하기 위해선 손실 함수에 절편에 대해 미분을 해주는 것이다.      자 다시!! 제곱 오차 함수의 최솟값을 얻기 위해 … 가중치에 대해 미분과 절편에 대해 미분을 해보자..    가중치에 대한 제곱 오차 미분을 하자   제곱 오차 식에 가중치(w)에 대해 편미분하면   $\\frac{\\delta SE}{\\delta w} = \\frac{\\delta}{\\delta w}(y - $y_hat$)^2 = 2(y -$ y_hat$)(-\\frac{\\delta}{\\delta w}$y_hat$) = 2(y - $y_hat$)(-x) = -2(y - $y_hat$)x$      정리하면 $\\frac{\\delta SE}{\\delta w} = -2(y - $y_hat$)x$      만약에 처음에 제곱 오차 공식을 (y-y_hat)$^2$가 아닌 2(y-y_hat)$^2$였다면 미분시 2와 $\\frac{1}{2}$이 곱해지면서 1이 되어 훨씬 깔끔하게 표현 되었을 것이다. 따라서 보통은 제곱 오차 공식을 2로 나눈 함수를 편미분 하여  $\\frac{\\delta SE}{\\delta w} = -(y - $y_hat$)x$    이와 같은 방식으로 나타내기도 한다. 앞으로는 이렇게 나타낼 것임    이제는 가중치에 대한 제곱 오차 함수의 변화율을 구하였으니 이전 예제 코드에서 가중치 갱신에 변화율을 더했던 방법과 비슷한 방법으로 가중치를 갱신할 것이다.  이 방법에서는 기존 w에서 변화율을 뺄 것이다.    이전 예제 코드와 다르게 왜 여기선 w에서 변화율을 더하지 않고 빼냐??   -&gt; 손실 함수의 낮은 쪽으로 이동하기 위해서   아니 무슨말 ??   이전에는 단순히 예측값이 기존 타깃값보다 작았기 때문에 예측값을 크게해주기 위해 가중치나 절편을 늘려야 했기 때문에 가중치와 절편에 각자의 변화율을 더해준 것 이라면 지금은 손실함수를 적용하였음   손실함수의 최솟값을 구하는 것이 곧 예측값과 타깃값을 유사하게 만드는 것이니 손실함수값을 낮추기 위해 가중치와 절편에 손실 함수에 대한 각자의 변화율을 빼주는 것이다. !!!   w = w - $\\frac{\\delta SE}{\\delta w}$ = w + (y - y_hat)x    위 식은 이전 예제 코드에서 오파 역전파를 알아보면서 보았던 식임…. (w + w_rate * err)  (y - y_hat)이 err고 err에 w_rate곱한 값에 기존 w를 더하여 갱신된 w를 만든다.      가중치와 마찬가지로 절편에 대하여 제곱 오차 미분을 하면    제곱 오차 식에 절편(b)에 대해 편미분하면   $\\frac{\\delta SE}{\\delta b} = \\frac{\\delta}{\\delta b}\\frac{1}{2}(y - $y_hat$)^2 = (y -$ y_hat$)(-\\frac{\\delta}{\\delta b}$y_hat$) = (y - $y_hat$)(-(b - $b_hat$)) = (y - $y_hat$)(-1) = -(y - $y_hat$)1$     b - b_hat = 1 인 것은 이전에 설명 하였으니 이해가 될 것이다   정리 하면 $\\frac{\\delta SE}{\\delta b} = -(y - $y_hat)     가중치에서 가중치에 대한 제곱 오차 함수의 변화율을 뺀 방식과 유사하게 절편에서 절편에 대한 제곱 오차 함수의 변화율을 빼면   b = b - $\\frac{\\delta SE}{\\delta b} = b + (y - $y_hat)       이제부터 손실 함수에 대해 일일이 변화율의 값을 계산하는 대신 편미분 이용하여 변화율 계산할 것임…    변화율 = Gradient     ''' 위에서 경사 하강법을 이용하여 회귀 문제를 변화율을 직접 구하는 방식, 편미분을 이용한 방식(손실 함수) 두 가지 방식을 이용하였다. 아래 예제는 두 가지 방법 모두 사용한 방식으로 파이선 클래스를 만들어 볼 것임 ''' class Neuron:   def __init__(self):     self.w = 1.0     self.b = 1.0    def forpass(self, x): #정방향 계산을 위한 메서드 정의     y_hat = x * self.w + self.b      return y_hat    #정방향 계산 : 뉴런으로 도식화한 상태에서 y_hat을 구하는 방향으로의 계산    def backprop(self, x, err): #역방향 계산을 위한 메서드 정의      w_rate = x       b_rate = 1      w_grad = w_rate * err  #이전에 설명한 경사 하강법 이용     b_grad = b_rate * err      return w_grad, b_grad    #역방향 계산 : 이전에 계산한 y_hat으로 정방향 계산과 반대로 y_hat과 y의 오차를 이용해 w와 b를 갱신   # 즉 정방향 계산과 반대의 방향으로 계산되는 것을 알 수 있다.    #따라서 오차역전파(backpropagation)이라는 용어가 나오게 된 것!!!!!    def fit(self, x, y, epochs = 100): #훈련을 위한 메서드 구현     for i in range(epochs):       for x_i, y_i in zip(x, y):         y_hat = self.forpass(x_i) #forpass 메서드 이용해 y_hat구함                  err = -(y_i - y_hat) #타깃값과 예측값의 오차를 구함          w_grad, b_grad = self.backprop(x_i, err) #backprop 메서드를 이용해 w, b를 갱신          self.w -= w_grad         self.b -= b_grad  neuron = Neuron() #위에서 만든 클래스에 대한 객체를 만들고 neuron.fit(x, y) # 만든 객체를 이용해 입력 data x 와 타깃 data y를 전달하여 훈련함  # 학습이 완료된 결과 (w, b를 이용한 선형 함수)를 그래프에 나타내 보자 plt.scatter(x, y) pt1 = (-0.1, -0.1 * neuron.w + neuron.b) pt2 = (0.15, 0.15 * neuron.w + neuron.b)  plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]])  plt.xlabel('x') plt.ylabel('y')  plt.show()      Reference    박해선, 딥러닝 입문, 이지스퍼블리싱, 2019, 45~74pg  ","categories": ["Deep_Learning"],
        "tags": ["선형 회귀","경사 하강법","손실 함수","오차 역전파"],
        "url": "/deep_learning/Numerical_prediction/",
        "teaser": null
      },{
        "title": "스프링 프레임 워크가 왜 생겼을까???",
        "excerpt":"스프링 이란??  스프링은 너무 많은 기능을 제공해 주고 있어 정의하기 쉽지가 않다.   스프링은 프레임워크인데 주요 기능 및 특징을 간단히 정리해보면     기술 :  1. 의존 주입 (DI, Dependency Injection) 지원  2. AOP (Aspect - Oriented Programming) 지원  3. MVC 웹 프레임워크 제공 등등      언어 :  자바, 코틀린, 그루비      이 외에도 수많은 기능 및 특징이 있다.   이렇게 방대하고 많은 스프링 프레임워크를 편리하게 사용할 수 있도록 지원해주는 것 또한 존재하는데     스프링 부트 :  스프링을 편하게 사용할 수 있도록 지원한다. 따라서 최근에는 기본으로 많이 사용 한다고 한다.   기능 :  1. 단독으로 실행할 수 있는 스프링 애플리케이션 쉽게 생성  2. Tomcat 같은 웹 서버 내장해서 별도로 설치 안해도 됨  3. 손쉬운 빌드 구성을 위한 starter 종속성 제공  4. 스프링과 외부 라이브러리 (3rd parth) 자동 구성 등등      이제 스프링에 대해 대충 알았으니 스프링을 왜 생겨났는지 알아보자     스프링이 필요로 하는 가장 큰 이유 :  좋은 객체 지향 애플리케이션을 개발할 수 있게 도와주는 역할을 하기 때문     스프링이 어떻게 좋은 객체 지향 애플리케이션을 개발할 수 있게 도와줄까???   그 전에 좋은 객체 지향 프로그래밍이 뭐지 ??       좋은 객체 지향 프로그래밍이 뭔데?     먼저 객체 지향 프로그래밍이 뭘까?     객체 지향 프로그래밍 (OOP, Object Oriented Programming) :  실세계에 존재하는 정보 혹은 상황을 “객체” 들의 모임과 “객체들 간의 메시지”를 주고받는 관계로 보자는 것이다.     즉 기존에 컴퓨터 프로그램을 단순히 명령어의 목록으로 보지말고 객체들의 모임과 객체들이 메시지를 주고 받으며 데이터를 처리한다고 보는 프로그래밍 이다.       이렇게 하면 뭐가 좋은데 ??      프로그램을 유연하고 변경이 용이하게 만들 수 있다.  따라서 대규모 개발에 많이 쓰인다.      하 이거 또 추상적인 표현이 나왔네… 프로그램을 유연하고 변경이 용이하게???? 이게 무슨 뜻??   이 뜻을 이해하기 위해선 객체 지향 프로그래밍 특징을 알아야 한다.      객체 지향 프로그래밍의 특징을 보면   1. 다형성 :  하나의 클래스나 메서드가 다양한 방식으로 동작 가능   대표적으로 오버라이딩과 오버로딩이 있다.    오버라이딩 :  부모 클래스의 메서드를 자식 클래스에서 재정의 오버로딩 :  이름이 같은 메서드가 매개변수의 자료형 혹은 수에 따라 다르게 정의 가능      단, 하위 클래스는 인터페이스 규약을 꼭 지켜야 한다. 즉 오버로딩, 오버라이딩 하면서도 인터페이스의 규약을 지키는 선에서 다르게 재정의를 해야한다.!!     2. 추상화 :  공통의 속성이나 기능을 가진 것들의 공통점을 일반화하여 묶고 각각의 세부적인 특징을 제거하여 단순하게 묶는 것이다. 객체 지향적 관점에서는 클래스를 정의하여 공통의 속성이나 기능 등을 묶는 것이다.      3. 캡슐화 :  클래스 내에 있는 정보들을 캡슐로 감싸서 숨기는 것을 의미한다. (클래스의 접근 지정자 이용)    이렇게 하여 외부에서 클래스 내의 정보들에 접근을 막아 정보 은닉을 할 수 있다.     4. 상속:  클래스와 클래스 사이의 관계를 의미하는데 클래스 간의 상속으로 엮이면 자식 클래스는 부모 클래스의 정보들을 상속 받을 수 있다.     등등이 있다.         -&gt; 이 특징으로 인하여 프로그램을 역할과 구현으로 구분하기 쉽고 그로 인해 프래그램을 유연하고 용이하게 만들 수 있기 때문이다.     쉽게 생각하면 역할이 인터페이스이고 구현이 인터페이스를 구현(정의)한 객체라고 생각할 수 있다.   역할과 구현으로 분리 ??   현재 스프링 수업을 듣고 있는 김영한 강사님께서는 이 역할과 구현의 분리를 실세계에서 비유해주셨는데 훨씬 이해가 잘되었다. 만약 운전자와 자동차가 있다고 가정하자.   운전자의 역할과 자동차의 역할이 각자 있을 것이다.     그리고 자동차 종류가 매우 다양한데  자동차 역할을 여러 종류의 자동차들 각각을 따로 구현을 해야한다.   만약 자동차가 k3, 아반떼가 있다고 하자 k3를 운전할 줄 아는 운전자가 아반떼를 운전할 수 있을까? -&gt; 당연하다    -&gt; 자동차의 종류가 바뀌어도 운전자에게 영향을 주면 안된다.!!       유연하고 변경 용이하다 -&gt; 운전자가 k3를 운전하다가 아반떼를 타고싶을 때 쉽게 운전할 수 있어야 한다.      어떻게 이게 가능?? 자동차의 역할(인터페이스)를 다 따라서 각 종류의 자동차를 구현하였기 때문에     운전자는 자동차의 역할에 대해서만 알고 있지 자동차 구현에 대한 것은 알지 못한다.    자동차의 구체적인 구현이 바뀌더라도 자동차의 역할은 바뀌지 않고 운전자 입장에서 자동차 구현이 바뀌더라도 운전할 수 있어야 한다.!!   운전자 역할 -&gt; 자동차 역할     자동차 역할 &lt;- 자동차 구현 : k3, 아반떼….     웹 서비스 입장에서 생각해보자 웹 서비스를 사용하는 클라이언트들은 웹은 구체적인 구현에 대해 알 필요가 없다. 만약 구현을 알아야만 서비스를 사용할 수 있다면  너무 끔찍하다… 그저 서비스를 사용하기만 하면 될 뿐이다.   마찬가지이다 운전자는 자동차의 구현에대해 알 필요가 없다. 아니 알고 싶지도 않다.. 자동차의 역할만 안다면 자동차 구현이 다르든 바뀌던 아니면 새로운 종류의 자동차가 나오더라도 운전자는 운전을 할 수 있다.!!   또한 클라이언트(운전자)에 영향을 주지 않으면서 새로운 기능을 제공할 수도 있다. 즉 구현이 바뀌더라도 클라이언트는 그것을 배울 필요가 없다.      이 모든 것을 역할과 구현으로 나누었기 떄문에 가능하다!!      또 다른 예시로는 내가 재밌게 본 드라마 나의 아저씨를 예로 들어보자!!   나의아저씨에서 남자 주인공 역으로 “박동훈”, 여자 주인공 역으로 “이지안”이 있다.   이 “박동훈”, “이지안”역을 역할로 볼 수 있다.      하지만 이 “박동훈” 역할은 이선균 배우가  맡을 수 있지만 다른 배우가 맡을 수도 있다. Ex) 조정석 배우   마찬가지로 “이지안” 역할은 아이유 배우가 맡을 수 있지만 다른 배우가 맡을 수도 있어야 한다. Ex) 전지현 배우   즉 각 역할을 다른 배우로도 대체가 가능하다. (가능 해야한다)   이렇듯 “박동훈”, “이지안” 역할은 역할로 구현은 “박동훈”을 이선균 배우로 할지 조정석 배우로할지 “이지안”을 아이유 배우가 할지 전지현 배우가 할지로 나누어져 있기 때문에 배우들을 대체 가능하다!!!  배우가 바뀌어도 역할은 바뀌지 않는다. “박동훈”역할을 이선균 배우가 하고 있고 “이지안”역할을 아이유 배우가 하고 있다가 아이유 배우 대신 전지현 배우가 대체하엿다해서 이선균 배우에게 영향을 전혀 끼치지 않는다. 그냥 대본에 따른 연기를 하면 된다.   -&gt; “박동훈” 역할이 클라이언트 , “이지안”역할이 서버라고 가정하면 “이지안” 역할의 배우가 바뀐다해서(구현이 바뀐다해서) 클라이언트에게 영향을 끼치지 않는다.       이 모든 예가 객체 지향의 특징으로 유연하고 변경이 용이함을 보여준다.  정리 해보자    역할과 구현을 분리하면     클라이언트는 대상의 역할(인터페이스)만 알면 됨.     클라이언트는 구현 대상의 내부 구조를 몰라도 됨.     클라이언트는 구현 대상의 내부 구조가 바뀌어도 영향 X.     클라이언트는 심지어 구현 대상 자체를 변경해도 영향 X.      -&gt; 프로그램이 단순해지고 유연해지며 변경 또한 편리해지는        스프링 프레임워크는 자바 언어로 구성 되어있다.   그러면 자바 언어로 역할과 구현을 나누어 보자       역할 : 인터페이스  구현 : 인터페이스를 구현(정의)한 클래스, 구현 객체    =&gt; 따라서 객체를 설계할때 인터페이스(역할)를 먼저 부여를 한 후 그 역할을 수행하는 구현 객체를 만들자.(정의 하자)       객체간의 협력 관계을로 생각하자   클라이언트 : 요청   서버 : 응답     이라고 보면 수 많은 객체 클라이언트와 객체 서버는 서로 협력 관계를 가지게 된다!!!   어떻게 자바에선 역할과 구현을 나누는지 생각해보자      객체 지향의 특성 중 다형성 그 중에서도 오버라이딩을 생각해보면 어떤 인터페이스(역할)가 있고 그 인터페이스를 구현(정의)한 객체를 실행 시점에서 유연하게 변경할 수 있다. -&gt; 유연, 변경 편리       즉 다형성 특징을 통해 (특히 오버라이딩 역할) 클라이언트를 변경하지 않고 서버의 구현 기능을 유연하게 변경할 수 있다.        역할과 구현을 분리 한다는 것을 정리 해보면     실세계에서 존재하는 정보를 역할과 구현이라는 편리한 컨셉을 객체지향의 특징들을 이용해 프로그램의 객체 세상으로 가져올 수 있다.     유연하고 변경이 용이하다.    설계의 확장이 가능하다.     클라이언트에게 영향을 주지 않으면서 내부 구조를 바꿀 수 있다.      -&gt; 단 인터페이스(역할)을 안정적으로 잘 설계해야 역할을 기준으로 구현을 할 수 있기 때문에 역할을 잘 설계하는 것이 중요하다.!!      하지만 역할과 구현을 분리함에 있어서 문제점, 한계점은 없을까??   역할과 구현의 분리의 문제점(한계점)    역할(인터페이스) 자체가 변해버리면 클라이언ㅌ와 서버 모두에 큰 영향을 미친다. -&gt; 클라이언트, 서버 모두 큰 변경이 발생.    ex) 역할 : 자동차에서 =&gt; 비행기로 바꾼다면 ???   구현도 모두 바뀔 뿐더러 (여러 비행기 종류가 있게지 ex) 보잉 등등), 운전자 역할 또한 변경됨. 당연하지 자동차랑 비행기는 완전히 달라지니까 운전 방법 또한 다르겠지        자 지금까지 스프링이 왜 생기게 되었냐?   -&gt; 좋은 객체 지향 애플리케이션을 개발할 수 있게 도와주는 역할     좋은 객체 지향이 뭘 해주길래??   -&gt; 프로그램을 유연하고 변경이 용이하게 만들 수 있다      프로그램을 어떻게 유연하고 변경이 용이하게 하지??   -&gt; 객체 지향 특징을 통해 역할과 구현을 분리      여기서 의문 좋은 객체 지향을 어떻게 하는지? 의문이 든다..    좋은 객체 지향 설계의 5가지 원칙 (SOLID)  1. SRP(Single Responsibility Principle) 단일 책임 원칙 :    한 클래스는 하나의 책임만 가져야 한다는 원칙이다.   여기서 책임이 무슨 책임을 뜻하는 건지 모르겠다… 거기다가 굳이 하나의 책임 ???  이 것은 문맥과 상황에 따라 다르게 해석되기 때문에 모호하다.   하지만 중요한 기준을 세워두면 상황에 따라 해석할 수 있는데  “변경”  이 그 기준이 된다.   프로그램 내에서 “변경”이 일어났을때 파급 효과 (영향)이 적다면 SRP를 잘 따르는 것으로 보면 된다.  ex) 한 클래스에 어떤 데이터를 읽는 기능과 어떤 데이터를 쓰는 기능 둘다 존재한다고 하면 만약 데이터를 읽는 기능에 “변경”이 일어났을때 이 클래스에 영향을 끼칠 것이다. 또한 데이터를 쓰는 기능에 “변경”이 일어 났을때에도 이 클래스에 영향을 끼칠 것이다. 따라서 클래스는 SRP를 지키지 않은 코드이고, 데이터를 읽는 기능, 쓰는 기능 두가지를 나눠서 각각의 클래스로 나눠야 SRP를 지킨 코드가 될 것이다.     2. OCP(Open / Closed Principle) 개방 / 폐쇄 원칙 :    소프트웨어 요소는 확장에는 열려 있으나 변경에는 닫혀있다는 원칙이다.   먼가 이상하다.. 확장에는 열려있다고 했다. 확장을 하려면 코드를 변경에야 할 것 아닌가?? 근데 변경에는 닫혀있다니.. 통 이해가 되지 않는다.   -&gt; 이 문제는 이전에 설명한 객체 지향의 특징 중 다형성으로 충분히 해결이 가능하다   만약 어떤 확장을 해야하는 상황이라면  인터페이스를 구현한 새로운 클래스를 하나 만들어서 새로운 기능을 구현한다면 기존 코드를 변경하지 않으면서 확장이 가능하기 때문이다. (이것 또한 “역할과 구현의 분리”로 가능하네..)     하지만 OCP에 큰 문제점이 있다. 아래 예를 들어 보자  ex)  public class A {    private B b = new C();  }    -&gt; 확장    public class A {  //private B b = new C();    private B b = new D();  }     A : 클라이언트 코드   B : 서버 코드   라고 하자     만약 A 클라이언트가 구현 클래스를 직접 선택한다고 하자   B b = new C(); =&gt; 기존 코드   B b = new D(); =&gt; 변경 코드 일때     구현 객체를 변경하려면 클라이언트 코드를 변경해야 하는 상황이다. -&gt; 예시 코드에선 분명히 다형성을 적용 (역할과 구현을 확실히 분리함) 했지만 OCP를 지킬 수 없는 상황이다.  이걸 어떻게 해결하지 ?….   -&gt; 객체 생성하고 연관관계를 맺어주는 별도의 조립, 설정자가 필요한 상황이다.   이걸… 구현하기엔 너무 복잡한데…    그래서 Spring 프레임 워크가 생겨난 것임… -&gt; 위 문제를 Spring Container가 해결 해준다..       3. LSP(Liskov Substitution Principle) 리스코프 치환 원칙:  프로그램의 객체는 프로그램의 정확성을 깨뜨리지 않으면서 하위 타입의 인스턴스로 바꿀 수 있어야 한다는 원칙이다.   이 원칙을 보면 확실히 이전에 설명한 객체 지향의 특성을 지키기 위한 원칙임을 알 수 있다.   다형성의 특징을 보면 하위 클래스는 인터페이스 규약을 꼭 지켜야 한다고 했는데 이 것을 의미하는 원칙이 LSP 이다.   ex) 쉽게 자동차로 예를 들면 엑셀을 밟으면 앞으로 가야하는 기능을 다형성 특징으로 구현을 다양하게 하는 경우에 더 빨리 앞으로 가게 한다던지, 조금 천천히 앞으로 가게 한다던지 등등 앞으로 가는 기능은 지켜야 한다. 하지만 옆으로 가거나 뒤로 가게 하는 구현을 하면 안된다는 것이다.   오버라이딩 입장에서 이런 기능을 나타내는 메서드를 재정의 할때 엑셀을 밟았을때 앞으로 가야하는 기본 인터페이스 규약은 꼭 지켜야 한다는 뜻이다.      따라서 LSP를 잘 지키면 이너페이스가 명확해지고, 대체 가능성이 높아지는 프로그램이 된다.!!!        4. ISP(Interface Segregation Principle) 인터페이스 분리 원칙:  특정 클라이언트를 위한 인터페이서 여러 개가 범용 인터페이스 하나보다 낫다는 원칙이다. 어떤 한 인터페이스가 여러개의 역할을 담고 있다면 각자의 역할이 서로 엮이게 되면서 서로 영향을 주게 된다. 따라서 하나의 역할이 변하면  다른 역할에 영향을 끼치기 때문에 문제가 된다.   따라서 인터페이스는 최대한 각각의 역할에 따라 분리가 되어야 한다.   ex)  자동차 인터페이스 -&gt; 운전 인터페이스와 정비 인터페이스로 분리 하고   사용자 클라이언트를 -&gt; 운전자 클라이언트와 정비사 클리이언트로 분리 한다면   만약에 정비 인터페이스가 변한다고 해도 운전자 인터페이스에 영향을 끼치지 않기 때문에 인터페이스 훨씬 명확해지는 장점이 있다.     즉 ISP를 지키면 인터페이스가 명확해지고 대체 가능성이 높아진다.!!!         5. DIP(Dependency Inversion Principle) :  프로그래머는 “추상화에 의존해야하고 구체화에 의존해선 안된다.”라는 원칙이다.   이전에 잠깐 언급된 의존성 주입은 DIP를 따르는 방법중 하나이다.  이 말을 다르게 표현하면 구현 클래스에 의존하지 말고 인터페이스에 의존 해라는 의미이다.!      즉 DIP는 이전에 설명한 “역할과 구현의 분리” 중에서도 “역할”에 의존해야 한다는 것이다.   객체 세상도 클라이언트가 인터페이스에 의존해야 유연하게 여러 구현을 할수 있기 때문이다 (구현체를 변경할 수 있음.) -&gt; 유연하고 변경에 용이.      여기서 큰 문제가 있다.!!   OCP를 설명할때 예에서 A는 인터페이스에 의존하지만 구현 클래스도 동시에 의존하고 있다.   A 클라이언트가 구현 클래스를 직접 선택 하기 때문이다.   그럼 이전에 설명한 예시는 DIP를 위반하는 것으로 볼 수 있다.      정리 하자면..     객체 지향의 특성으로 개발이 편리하다.     다형성의 특성으로 구현 객체를 변경할 때 클라이언트 코드도 변경되는 문제가 존재했다.    -&gt; 따라서 다형성의 특성만으로는 OCP, DIP를 지킬 수 없었다. 이러한 문제를 해결할 방법을 찾아야 한다.      Spring에 대해 배우고자 하였지만 지금까지 Spring에 대한 이야기가 거의 없었다…   하지만 지금 까지 Spring이 왜 생겨 났는지를 알기 위해 이야기를 이어 왔다.   위의 정리 2에서  다형성의 특징만으로는 OCP, DIP를 지킬 수 없었다.  이 문제를 해결하기 위해 Spring이 생겨났다.     즉 OCP, DIP를 지키지 못했다는 것은 좋은 객체 지향 프로그래밍을 하지 못했다는 것이고 OCP, DIP를 지켜서 좋은 객체 지향 프로그래밍을 하게끔 도와주기 위해 Spring이 생겨난 것이다.      그럼 어떻게 Spring으로 OCP, DIP를 가능하게 하지??   스프링에서 OCP, DIP를 가능하게 하는 기술 :  1. DI(Dependency Injection) 의존관계, 의존성 주입  2. DI container 제공      -&gt; 이 기술로 클라이언트 코드의 변경없이 기능이 확장이 가능하게 된다. 따라서 쉽게 부품을 교체하듯이 개발이 가능하게 된다.!!!   위 DI에 대해서는 앞으로 차차 알아가게 될 것이다.       Spring이 존재하기 이전에 개발자들은 좋은 객체 지향 개발을 하기 위해서 특히 OCP, DIP를 지키면서 개발을 하기 위해서 너무 많은 고생을 하게 되었고 자연스럽게 이러한 문제를 해결하기 위해 Spring 프레임워크가 생기게 되었다. (정확히는 DI container가 생기게 되었다.)       지금 까지 Spring이 왜 생겨나게 되었는지 알게 되었다.     이후에는 Spring이 왜 생겨났는지 코드를 통해 느껴보고자 한다.     Reference :  김영한 강사님 ㅅ프링 핵심 원리 - 기본편  강의 중  ","categories": ["Spring"],
        "tags": ["스프링 프레임워크","스프링 부트","객체 지향 프로그래밍","SOLID"],
        "url": "/spring/spring_basic(1)/",
        "teaser": null
      },{
        "title": "이진 분류",
        "excerpt":"이진 분류(Binary Classification) :  임의의 샘플 데이터를 참 혹은 거짓으로 구분하는 문제를 말한다.   퍼셉트론(Perceptron) :  이전에 공부한 선형 회귀와 매우 유사함 하지만 퍼셉트론은 마지막 단계에서 샘플을 이진 분류하기 위해 계단 함수를 사용한다.   이후 계단 함수를 통과한 값을 다시 가중치와 절편을 갱신하도록 학습하는데 사용한다.       만약 뉴런은 입력 신호들을 받아 z를 만든다고 해보자  $w_1x_1 + w_2x_2 + b = z$라 하자  아니 이전에 공부한 선형 방정식과 다른 것 같은데 -&gt; 이전에는 wx + b = y였는데 이 것은 입력값이 한개 더 늘었을 뿐이다. (원래 x 한개에서 $x_1과x_2$ 2개의 featrue가 입력값)      자 이제 선형 함수의 출력값인 z를 계단함수에 적용해보자.. 그런데 계단함수가 뭐지?   계단 함수(step function) :  z가 0보다 크거나 같으면 1로 0보다 작으면 -1로 분류하는 함수  y = 1 (z &gt;= 0)  y = 0 (z &lt; 0)        image_reference : aistudy.co.kr/neural/activation_function.htm      정리 하자면 퍼셉트론은 선형 함수를 통과해서 얻은 결과값인 z를 계단함수에 입력하여 그 값이 0보다 클지 작을지에 대해 검사하여 크다면 1을 작다면 -1로 분류하는 간단한 알고리즘 이다.     퍼셉트론은 계단 함수의 결과를 사용하여 가중치와 절편을 업데이트 한다.      현재부터 여러 개의 특성(feature)을 사용해 보자  그러면 특성이 n개인 선형 함수를 나타내보면    $y=w_1x_1=w_2x_2+~~~+w_nx_n+b$         이 식을 sigma기호로 나타내면   $b + \\sum_{i=1}^{n} w_ix_i$     늘어난 입력 특성에 따라 가중치(w) 갯수도 따라 늘어나는군… 당연하지 입력(x)에 곱해지는 값이 가중치인데 물론 절편(b)은 1개 그대로임     지금까지 퍼셉트론에 대해 알아 보았다.  지금까지 퍼셉트론을 모두 구현해 왔지만 사이킷런 패키지에서 Perceptron이라는 이름의 클래스를 제공해 주기 때문에 이 것을 이용하면 직접 구현할 필요가 없다.   그럼 지금까지 뭐한거 ?? 어떻게 해당 클래스가 구성되어있는지 알면 그 클래스를 사용하거나 이해하기 좋으니까..   아달린(Adaline) :  퍼셉트론을 개선한 적응형 선형 뉴런(Adaptive Linear Neuron)       적응형 선형 뉴런 ??? 그냥 선형 뉴런과 무슨 차이 ??     아달린에 대해 알아보면 아달린은 선형 함수의 결과를 학습하는데 사용한다 (선형 함수의 가중치와 절편 갱신하는 학습). 그리고 계단 함수의 결과는 예측에만 활용한다….     이전에 배운 선형 뉴런 경우 선형 함수의 결과를 계단 함수에 입력하고 그 결과를 이용해 학습과 예측을 하였다. 하지만 아달린 경우에는 역방향 게산이 계단 함수 출력 이후가 아닌 선형 함수의 출력 이후에 진행된다는 차이점이 있다.        그런데 아달린을 하면 뭐가 더 좋아지나 보군.. 어떤것이??   아달린을 좀더 개선한 버전이 로지스틱 회귀이다.   로지스틱 회귀가 이전 선형 뉴런에 비해 어떤 점이 좋은지 알 수 있으면 위의 의문이 해결될 것이다.         로지스틱 회귀(logistic regression) :  아달린에서 좀더 발전한 형태이다.   로지스틱 회귀는 선형 함수를 통과시켜 얻은 z를 임계 함수에 보내기 전에 변형시키는데 이 변형 시켜주는 함수가 활성화 함수(activation function)이다.   선형 함수를 통과하여 얻은 z가 활성화 함수에 들어가여 그 결과로 a를 내놓는다고 하자 그리고 이 a는 임계 함수에 들어가 예측값을 얻게 된다.   또한 아달린과 유사하게 (좀 다름) a가 역방향 계산이 일어나 선형함수의 결과를 학습하는데 사용된다 (적합한 가중치와 절편 찾는 학습)   임계 함수는 아달린에서의 계단 함수와 역할 비슷하지만 활성화 함수의 출력값을 사용한다는 점이 차이다.       각 차이점을 정리 해보자    퍼셉트론 (일반 선형 뉴런) :  입력 -&gt; 선형 함수 -&gt; z -&gt; 계단 함수 -&gt; y_hat   예측 값인 y_hat가 다시 역방향 계산이 일어나 선형 함수의 결과를 학습하는데 사용 (적합한 가중치와 절편 찾는 학습)      아달린 (적응형 선형 뉴런) :  입력 값 -&gt; 선형 함수 -&gt; z -&gt; 계단 함수 -&gt; y_hat   선형 함수의 결과값인 z가 다시 역방향 계산이 일어나 선형 함수의 결과를 학습하는데 사용 (적합한 가중치와 절편 찾는 학습)      로지스틱 회귀 :  입력값 -&gt; 선형 함수 -&gt; z -&gt; 활성화 함수 -&gt; a -&gt; 게단 함수 -&gt; y_hat   활성화 함수의 결과값인 a가 다시 역방향 계산이 일어나 선형 함수의 결과를 학습하는데 사용 (적합한 가중치와 절편 찾는 학습)      활성화 함수는 비선형 함수를 사용해야 한다.!!  왜 활성화 함수로는 선형함수가 아닌 비선형 함수를 사용해야 하지 ??   ex) 만약 선형함수인 $y = w_1x_1 + w_2x_2 + …. + w_nx_n$에다가 활성화 함수 y = ka (선형 함수) 가 있다고 하자.   이 둘을 쌓은 수식은 $y = k(w_1x_1 + w_2x_2 + … + w_nx_n)$이 되고 이 결과는 다시 선형함수가 된다. 이렇게 되면 임계 함수 앞에 뉴런을 아무리 많이 쌓는다고 해도 그 결과가 선형함수일 것이고 그로 인해 큰 의미가 생기지 않게된다. 따라서 활성화 함수로 비선형 함수를 사용하는 것이다.       비선형 함수의 예로 $p = \\frac{1}{1 + e^{-z}}$ 이 있으며 이 외에도 다양한 함수가 존재한다.      그러면 로지스틱 회귀에서 사용되는 활성화 함수는 어떤 함수일까?   -&gt; 로지스특 회귀에서는 “시그모이드 함수(sigmoid function)”를 활성화 함수로 사용한다.       시그모이드 함수가 어떤 역할을 하며, 어떤 과정으로 만들어지겓 되었는지 알아보자   로지스틱 회귀에서 선형함수의 출력값 z는 $z = b + \\sum_{i=1}^{n} w_ix_i$인데 이 z 값을 활성화 함수를 통과시켜서 a가 되게 된다. 이때 시그모이드 함수는 z를 0 ~ 1 사이의 확률값으로 변환시켜주는 역할을 하게 한다.   예를들어 분류를 해야하는 경우에 시그모이드 함수의 결과인 a가 0.5(50%)보다 크면 양의 클래스, 그 이하가 되면 음의 클래스로 분류를 한다.      그럼 시그모이드 함수가 어떻게 만들어 지는지 과정에 대해 알아보자     오즈 비 -&gt; 로짓 함수 -&gt; 시그모이드 함수     오즈 비(odds ratio) :  성공 확률과 실패 확률의 비율을 나타내는 통계    OR(odds ratio) = $\\frac{p}{1 - p}$ (p = 성공 확률)    오즈피를 그래프로 나타내면      image_reference : https://kau-deeperent.tistory.com/90      로짓 함수(logit function) :  오즈 비에 로그함수를 취하여 만든 함수    logit(p) = $log(\\frac{p}{1 - p})$    로짓 함수는 p = 0.5일때 0이 되고 p가 0과 1일때 각각 무한대로 음수와 양수가 되는 특징을 가진 함수이다.   그래프로 나타내보면     image_reference : https://i-am-eden.tistory.com/21   로짓 함수의 세로 축을 z로 가로축을 p로 놓으면 확률 p가 0 에서 1까지 변할 때 z가 매우 큰 음수에서 매우 큰 양수로 증가하는 것을 볼 수 있다.   따라서 이 식을 다시 정리하면  z = log($\\frac{p}{1 - p}$)       로지스틱 함수(Logistic function) :  위 로직 함수 z에 대해 아래와 같이 정리하면 로지스틱 함수가 된다.    $p = \\frac{1}{1 + e^{-z}}$  이 식을 유도한 방법은 기존 로짓함수의 식에서 z = ~ 꼴이 아닌 p = ~꼴로 바꾸어 주는 것이다.  이렇게 정리한 이유는 기존 로잣함수의 가로 축이 p 였는데 가로 축을 z로 놓기 위해서 이다. (역함수 변환과 유사)      image_reference     이렇게 나타낸 로지스틱 함수를 그래프로 그려보자    로짓 함수의 가로와 세로축을 반대로 뒤집은 모양이 된다. -&gt; 로짓 함수의 역함수가 로지스틱 함수 (y = x 그래프 기준 대칭으로 보면 됨.)   그리고 이 그래프는 S자 형태를 띈다.    이 S자 형태를 착안해서 로지스틱 함수를  시그모이드 함수(Sigmoid Function) 이라고 부른다.      로지스틱 회귀를 정리 해보면   선형 함수 -&gt; z -&gt; 로지스틱 함수 -&gt; a -&gt; 임계 함수 -&gt; y_hat   a는 역방향 계산으로 선형함수의 가중치와 절편을 갱신함.   이때 z는 $-\\infty$ ~ $\\infty$의 범위를 가지는데 로지스틱 회귀는 이진 분류가 목적이기 때문에 z 범위를 조절해야 한다. 이 문제의 해결을 위해 시그모이드 함수를 활성화 함수로 사용하여 0 ~ 1사이의 값으로 범위를 조절하였다.   따라서 시그모이드 함수의 결과인 a값이 0 ~ 1 로 확률처럼 해석이 가능하게 되었다. a를 확률로 이해하면 a를 0과 1로 구분 (이진 분류) 하기 위해서 마지막에 임계 함수를 이용하였다.  임계 함수에서 a값이 0.5보다 큰지 혹은 이하인지에 따라 1 혹은 0으로 y_hat으로 결과를 내놓았다.     지금 까지 이진 분류를 어떻게 하는지는 알겠는데 그럼 a 값으로 역방향 계산을 통해 선형함수의 가중치와 절편을 어떻게 갱신하는지 궁금하다…   이 방법도 손실함수를 사용할 것이다. 그러면 로지스틱 회귀에서는 어떤 손실 함수를 사용해 가중치와 절편을 갱신할 수 있을까??   이전에 배운 선형회귀에서 손실 함수로 제곱 오차를 사용한 것처럼 로지스틱 회기에서도 적용 안되려나 ??   이전에 배운 선형회귀는 정답과 예상값의 오차 제곱이 최소가 되느 가중치와 절편을 찾는 방법을 이용하였다 (제곱 오차) 하지만 로지스틱 회귀에서는 선형 회귀와 목적이 다르다. 선형 회귀는 어떤 값을 찾는 거라면 로지스틱 회귀는 분류가 목적이다!!!   즉 올바르게 분류를 하는 비율을 높이는 것이 목표이다.       이전에 배운 방법인 경사 하강법의 손실함수를 사용하려고 하니 … 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니기 때문에 이 방법은 불가능 하다. 그러면 다른 함수를 사용해야 한다..   -&gt; 이 함수가 바로 “로지스틱 손실 함수” 이다.     로지스틱 손실 함수 :  다중 분류를 위한 손실 함수인 크로스 엔트로피 (Cross Entropy) 손실 함수를 이진 분류 버전으로 만든 것   크로스 엔트로피 손실 함수는 이후에 배울 것이다.    L = -(ylog(a) + (1 - y)log(1 - a))      로지스틱 회귀는 이진분류에가 목적이기 때문에 y값 (타깃값)이 0 혹은 1이다. 따라서 위 손실함수 식에서 y값이 0 혹은 1이 되다.      y가 0인 경우 (음성 클래스) -&gt; L = -log(1 - a)   y가 1인 경우 (양성 클래스) -&gt; L = -log(a)      위 두 식의 값을 최소로 만들다 보면 a의 값이 원하는 목표의 값에 가까워 진다는 것을 알 수 있다.     음성 클래스 경우 (y = 0 경우) 로지스틱 손실 함수의 값(L)을 최소로 만들려면 a는 0에 가까워 지게 되고    양성 클래스 경우 (y = 1 경우) 로지스틱 손실 함수의 값(L)을 최소로 만들려면 a는 1에 가까워 지기 때문이다.     이 값들 (다시 을 계단 함수에 통과시키면 올바르게 분류 작업이 수행 하게 된다.        정리 하자면 로지스틱 손실 함수를 최소화 하면 a 값이 우리가 원하는 값이 되게 된다.     로지스틱 손실 함수의 미분   로지스틱 회귀의 가중치와 절편을 갱신하하기 위해 로지스틱 손실 함수를 미분해보자  미분 하는 이유는 나중에 알 수 있다.    가중치와 절편에 대한 로지스틱 손실 함수의 미분 결과는     $\\frac{\\delta}{\\delta w_i}L = -(y-a)x_i$    $\\frac{\\delta}{\\delta b }L = -(y-a)x_i$     이전 선형회귀 에서의 가중치와 절편에 대한 미분 결과와 유사하다   다만 y_hat 대신에 a가 있을 뿐이다.        로지스틱 회귀의 구현이 사실 선형 회귀와 큰 차이가 없는 것 같다.       우리는 로지스틱 손실함수의 가중치에 대한 미분, 절편에 대한 미분을 해야하는데  이 미분을 어떻게 해야할지 고민이다. 왜냐하면 로지스틱 손실함수(L)을 가중치(w)나 절편(b)로 바로 미분하는 것은 너무 복잡하기 때문이다.      미분의 연쇄 법칙을 이용한다면 이 문제를 해결할 수 있다.    미분의 연쇄법칙 :  $\\frac{\\delta y}{\\delta x} =\\frac{\\delta y}{\\delta u} \\frac{\\delta u}{\\delta x}$        미분의 연쇄 법칙을 적용하기 이전에 로지스틱 회귀의 과정을 다시 정리해보자  그 이유로는 역방향 계산 방향을 알아야 미분의 연쇄법칙을 적용할 수 있기 때문이다.     입력($x_1, x_2, … x_n$) -&gt; 선형 함수($w_1, w_2, —, w_n$, b) -&gt; z -&gt; 활성화 함수 -&gt; a -&gt; 계단함수 -&gt; y_hat      a에서 역방향 계산을 하여 선형 함수의 가중치와 절편을 갱신하고 그렇게 되면 더욱 타깃에 적합한 z-&gt;a-&gt;y_hat 값을 찾게 될 것이다.      이 과정을 보면 로지스틱 손실 함수에 대한 미분이 연쇄 법칙에 의해 진행되는 구조이다.   이러한 구조를 &lt;/b&gt; Gradient가 역전되었다 &lt;/b&gt;라고 표현한다.   그럼 이제 다시 본론으로 돌아와 미분의 연쇄법칙을 적용해 로지스틱 손실함수의 가중치, 절편에 대한 미분을 해보자     먼저 로지스틱 손실함수(L)을 활성화 함수의 출력값(a)에 대해 미분하고  활성화 함수의 출력값(a)은 선형 함수의 출력값(z)에 대하여 미분하고   선형 함수의 출력값(z)은 가중치(w) 또는 절편(b)에 대해 미분한다.   이 미분한 결과들을 서로 곱해주면 원하는 로지스틱 손실함수의 가중치 혹은 절편에 대해 미분한 결과가 된다.   이 과정이 미분의 연쇄법칙을 이용한 것이다.      이제 다시 정리해보면   로지스틱 손실 함수를 가중치에 대한 미분 :  $\\frac{\\delta}{\\delta w_i }L = \\frac{\\delta L}{\\delta a }\\frac{\\delta a}{\\delta z }\\frac{\\delta z}{\\delta w_i }$      로지스틱 손실 함수를 절편에 대한 미분 :  $\\frac{\\delta}{\\delta b}L = \\frac{\\delta L}{\\delta a }\\frac{\\delta a}{\\delta z }\\frac{\\delta z}{\\delta b }$      위 두 미분 계산 과정을 생략하고 식을 정리하면  $\\frac{\\delta}{\\delta w_i }L = -(y - a)x_i$  $\\frac{\\delta}{\\delta b}L = -(y - a)1$  이다.      이제 미분을 완료 하였으니 가중치와 절편을 갱신해보도록 하자.     로지스틱 회귀의 가중치 갱신  로지스틱 회귀의 가중치를 갱신하기 위해 로지스틱 손실 함수를 가중치에 대해 미분한 식을 가중치에서 뺀다.  $w_i = w_i - \\frac{\\delta L}{\\delta w_i} = w_i + (y-a)x_i$   로지스틱 회귀의 절편 갱신  가중치 갱시과 유사한 방법으로  $b = b - \\frac{\\delta L}{\\delta b} = (y-a)1$      이전에 가중치, 절편 갱신을 위해 왜 로지스틱 손실함수에 가중치와 절편에 대해 미분해야 했는지 이유를 위의 갱신하는 방법에서 쓰이기 때문인것을 알 수 있다. -&gt; 로지스틱 손실함수에 대한 가중치와 절편의 변화율을 이용   이전 선형 회귀에서 경사하강법을 기억할 것이다. 그때 왜 가중치(w), 절편(b)의 변화율을 이용하여 절편과 가중치를 갱신하는 이유를 설명하였다.      위에서 설명한 이진 분류를 이젠 직접 데이터를 이용해 구현해볼 것이다.   유방암 데이터를 이용할 것인데 이 유방암 데이터에는 특징이 10개가 존재한다.  평균, 표준오차, 최대 이상치 등이 잇는데 이를 이용해 유방암 데이터 샘플이 악성 종양(True)인지 정상 종양(False) 인지 판단해 볼 것이다. (이진 분류)     # 유방암 데이터 세트를 준비한다. from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer()   print(cancer) # cancer를 보면 처음에 입력 데이터, 두번째 0, 1로 타겟 데이터, 세번쨰는 뭐지??   {'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,         1.189e-01],        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,         8.902e-02],        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,         8.758e-02],        ...,        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,         7.820e-02],        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,         1.240e-01],        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,         7.039e-02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), 'target_names': array(['malignant', 'benign'], dtype='&lt;U9'), 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.', 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',        'mean smoothness', 'mean compactness', 'mean concavity',        'mean concave points', 'mean symmetry', 'mean fractal dimension',        'radius error', 'texture error', 'perimeter error', 'area error',        'smoothness error', 'compactness error', 'concavity error',        'concave points error', 'symmetry error',        'fractal dimension error', 'worst radius', 'worst texture',        'worst perimeter', 'worst area', 'worst smoothness',        'worst compactness', 'worst concavity', 'worst concave points',        'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23'), 'filename': '/usr/local/lib/python3.7/dist-packages/sklearn/datasets/data/breast_cancer.csv'}   # 입력 데이터를 확인해 보자. print(cancer.data.shape, cancer.target.shape) #입력 데이터의 data 크기를 출력한다.  #569개 샘플있고 30개의 특성이 있음을 확인할 수 있다.   (569, 30) (569,)   cancer.data[:3] #샘플 3개만 봐보자   #특성들이 실수 범위에 양수값이네 정도만 알 수 있다.  #이전 선형 회귀 처럼 산점도로 나타내서 특성간의 관계에 대해 보고 싶은데... 이게 30개나 되니까 산점도로 표현하기는 힘들듯..   array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,         3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,         8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,         3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,         1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],        [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,         8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,         3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,         1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,         1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],        [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,         1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,         4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,         2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,         1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02]])   import matplotlib.pyplot as plt  plt.boxplot(cancer.data) plt.xlabel('feature') plt.ylabel('value') plt.show()   # 산점도로 나타내기 힘드니 박스플롯으로 나타내 보았다. (박스플롯은 나중에 설명)  # 박스 플롯을 보면 4, 14, 24번째 특성에 대한 value값이 다른 특성들보다 분포가 훨씬 넓게 퍼저있음을 알 수 있다. # -&gt; 왜 이런지 한번 알아보자       cancer.feature_names[[3, 13, 23]]  # 4, 14, 24번째 (인덱스는 0부터니 코드랑 햇갈리지 말길)의 특성을 보면 # area : 넓이와 관련된 것임을 알 수 있다.  # 아 ... 주어진 데이터에서 세포들의 넓이가 다양하나 보다..    array(['mean area', 'area error', 'worst area'], dtype='&lt;U23')   박스 플롯 :     image_reference : http://demo.riamore.net/HTML5demo/chart/Docs/User%20Manual%20-%20html/box-plot-chart.html       위 그림에서 하위 표본 퀀타일이 1사분위, 표본 미디언이 2사분위, 사위 표본 퀀타일이 3사분위로 보면 된다.   박스 플롯은 1사분위와 3사분위 값으로 상자를 그리고 그 안에 2사분위값을 나타낸다.   1사분위와 3사분위 사이의 거리의 1.5배만큼 위아래 거리를 그려놓은 것이다.     import numpy as np  np.unique(cancer.target, return_counts = True) #타깃 데이터 확인  # 0과 1로만 우리어져 있고 0은 음성클래스 1은 양성클래스를 나타냄  # numpy의 unique()함수는 고유한 값을 찾아 반환하는 것이고 # 이때 return_counts = True로 매개변수 지정을 하면 고유한 값이 얼만큼 있는지 횟수를 반환 한다.  # -&gt; 결과를 통해 음성클래스(정상 종양) 212개, 양성클래스(악성 종양) 357개가 있음을 알 수 있다.  print(357 / 212)  # 양성클래스 : 음성클래스 비율이 대충 1.7 : 1 로 확인된다. -&gt; 이 비율 기억해두길 !!!    1.6839622641509433   # 훈련 데이터 세트 저장 x = cancer.data y = cancer.target    이전에 선형 회귀로 뉴런 클래스 만든 것 기억해보면 훈련 데이터 세트를 주어진 데이터 전체를 이용하여 모델을 훈련했다.   사실 웃긴 일이다. 모든 데이터로 훈련을 하면 어떤 데이터로 모델이 잘 훈련 되었는지 알 수 있을까?   훈련 데이터로 다시 훈련이 잘 되었는지 검증하는 것은 ??   예전에 읽은 책 중에 “혼자 공부하는 머신러닝, 딥러닝”에서 한 예시가 기억이 난다.   어떤 사람에게 특정 과목에 대해 잘 알 수 있게 훈련 문제들로 훈련을 시키고 과연 잘 공부 되었는지 확인하기위해 시험을 보았는데 시험 문제로 이전의 훈련 문제들로 시험문제를 낸다면 ?   해당 과목의 전체적인 공부가 잘 되지 않았더라도 훈련 문제만을 기억해서 풀어 모두 맞출수도 있기 떄문에 훈련이 잘 되었는지 제대로 확인할 수 없다.   이 예시와 마찬가지로 훈련 데이터로 검증을 한다?? 정확한 검증을 할 수 없다.    그러면 훈련 데이터의 모든 데이터를 모델을 훈련 시키기 위한 훈련 세트와 모델이 잘 훈련이 되었는지 검증할 테스트 세트로 나누면 될 것이다.     이렇게 나눈 것을  훈련 세트(training set)  테스트 세트(test set)  라고 한다.      그런데 어떻게 나눌까?? 그냥 주어진 데이터 반으로 딱 나누면 안되나?   -&gt; 아니다 훈련 데이터 세트를 훈련 세트와 테스트 세트로 나누는 2가지 규칙이 존재한다.     훈련 데이터 세트를 훈련 세트와 테스트 세트로 나누는 규칙     훈련 데이터 세트를 나눌 때는 테스트 세트보다 훈련 세트가 많아야 함    훈련 데이터 세트를 나누기 전에 양성 클래스와 음성 클래스가 훈련 세트나 테스트 세트 한쪽에 몰려선 안된다. (편향이 있어선 안된다.)       이러한 규칙을 지키지 않ㅇ면 데이터에 있는 패턴을 제대로 학습하지 못하기 때문에 잘못된 측정을 할 수 있다.      하… 이런 규칙을 지키면서 데이터 나누기 너무 귀찮은데..   -&gt; 사이킷런의 도구를 이용하면 편ㄴ하다.     사이킷런을 이용해 훈련 데이터 세트를 훈련 세트와 테스트 세트로 나누어 보자.   # 사이킷런을 이용해 훈련 데이터 세트를 훈련세트와 테스트 세트로 나누어 보자 from sklearn.model_selection import train_test_split   # sklearn.model_selection 모듈에 있는 train_test_split() 함수는 기본적으로 입력된 훈련 데이터 세트를 훈련 세트를 75%, 테스트 세트를 25% 비율로 나눈다.    x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.2, random_state = 42)  # 매개 변수에 대해서 알아보면   # 1. stratify = y : # stratify는 훈련 데이터를 나눌 때 클래스 비율을 동일하게 하는 매개변수이다. # train_test_split은 default로 데이터를 나누기 전에 섞지만 일부 클래스 비율이 균형이 맞지 않은 경우에 stratifyfmf y로 지정해야 한다.  # 2. test_size = 0.2 : # train_test_split()함수는 default로 훈련 데이터 세트를 75 : 25 비율로 나눈다. # 하지만 test_size 매개변수를 이용하면 이 비율을 조절할 수 있다. # 이 예제처럼 test_size = 0.2로 하면 훈련 데이터 세트를 80 : 20 비율로 나눌 수 있다.  # 3. random_state = 42 : # train_test_split() 함수는 무작위로 데이터 세트를 섞은 다음 나누는데 (랜덤하게),  # 이때 random_state는 rando seed를 세팅한다. # 랜덤 값은 사실 엄격한 랜덤값이 아니다.  # 어떤 특정한 시작 숫자를 정해 주면 컴퓨터가 정해진 알고리즘에 의해 마치 난수처럼 보이는 수열을 생성한다.  # 이런 시작 숫자를 시드(seed)라고 한다. # 예제와 같이 random_state = 42로 난수 초기값을 지정해 두면 데이터 세트를 섞는 결과가 항상 일정하게 나타난다.     # 훈련 데이터 세트가 잘 나누어졌는지 훈련 세트와 테스트 세트의 비율을 확인해 보자 print(x_train.shape, x_test.shape)  print(455 / 114)  # 결과를 확인해 보니 훈련 세트와 테스트 세트 80 : 20 비율로 잘 나누어져 있는것을 확인할 수 있다.   (455, 30) (114, 30) 3.991228070175439   # numpy의 unique()함수를 이용해서 훈련 세트의 타깃 안에 있는 클래스의 갯수를 확인해 보자.  np.unique(y_train, return_counts = True)  print(285 / 170)   #  확인해보니 전체 훈련 데이터 세트의 클래스 비율과 거의 비슷한 것을 알 수 있다. #  -&gt; 양성클래스 : 음성클래스 비율이 약 1.7 : 1 정도 됨   #  이전에 훈련 데이터를 나누기전에 양성클래스 : 음성클래스 비율을 기억해두라 한 적이 있다. 그 이유가 그때도 비율이 약 1.7 : 1 #  정도 였는데 train_test_split()함수가 훈련 데이터 세트를 훈련 세트와 테스트 세트로 나누는 규칙인 훈련 중 #  훈련 데이터 세트를 나누기 전에 양성, 음성 클래스가 훈련 세트나 테슽 ㅡ세트의 어느 한쪽에 몰리지 않도록 골고루 섞어야 한다는 규칙을  #  잘 지킨 것으로 확인된다.    1.6764705882352942   # 지금까지 배운 로지스틱 회귀를 이 데이터로 구현해보자  class LogisticNeuron:      def __init__(self):     self.w = None     self.b = None      # -&gt; 생성자에서 특이한점은 가중치와 절편을 미리 초기화 하지 않는다는 점이다.     # 그 이유는 입력 데이터의 특성 갯수를 알지 못하므로      # 가중치를 입력데이터가 들어오고 난 후 특성 개수에 맞게 결정    def forpass(self, x): # 선형 방정식 계산 (로지스틱 회귀에서 데이터가 정방향으로 흘러가는 과정)     z = np.sum(x * self.w) + self.b     return z    def backprop(self, x, err): # 역방향 계산 (가중치와 절편을 업데이트 하기 위해 데이터가 역방향으로 흘러가는 과정)     w_grad = x * err # 가중치에 대한 gradient 계산     b_grad = 1 * err # 절편에 대한 gradeint 계산      return w_grad, b_grad  # 훈련하는 메서드를 구현해보자  # 훈련 수행하는 fit() 메서드를 만들 것인데 이전 선형 회귀에서 구현한 Neuron 클래스에서 만든 함수와 같다 # 하지만 차이점으로는 activation()메서드가 추가되었음 -&gt; 당연 선형회귀와 로지스틱회귀의 차이점을 생각하면 알 수 있음    def fit(self, x, y, epochs = 100):     self.w = np.ones(x.shape[1]) #가중치를 초기화 한다. np.ones()함수를 이용해 1로 초기화     self.b = 0 #절편을 0으로 초기화      for i in range (epochs): # epochs만큼 반복       for x_i, y_i in zip(x, y):          z = self.forpass(x_i) # 선형함수식에 대입하여 z 구함          a = self.activation(z) # z를 활성화 함수에 대입하여 a 구함          err = -(y_i - a) # 오차를 계산함          w_grad, b_grad = self.backprop(x_i, err) # 역방향 계산으로 가중치 gradient와 절편 gradient 구함          self.w -= w_grad  # 가중치를 갱신한다.         self.b -= b_grad # 절편을 갱신한다.  # activation() 메서드를 만들어보자 # 로지스틱 회귀에서 사용할 활성화 함수는 시그모이드 함수이다. 따라서 시그모이드 함수를 구현하면 된다.    def activation(self, z):     a = 1 / (1 + np.exp(-z))      return a  # 예측값을 구해보자 (y_hat 구하기)  # 이전 선형 회귀 Neuron 클래스 구현에서 새로운 샘플에 대한 예측값 구할때 forpass() 메서드 사용했던것 기억할 것이다. # 근데 만약 에측해 볼 샘플이 많다면 forpass()메서드를 계속 호출해야하는 귀찮은 일이 생긴다. # 거기다가 로지스틱 회귀에서는 활성화 함수와 임계 함수까지 추가되니 어휴... 정말 귀찮다.  # -&gt; 이러한 문제를 해결해줄 predict()메서드를 만들어 보자 # 샘플에 대한 예측값을 계산해주는 predict() 메서드를 만들어 보자    def predict(self, x):     z = [self.forpass(x_i) for x_i in x] # 리스트 컴프리헨션으로 반복문으로 여러 샘플을 선형 함수에 대입 후 그 결과를 리스트에 저장      a = self.activation(np.array(z)) # 활성화 함수에 z 리스트 값 대입      return a &gt; 0.5 # 계단 함수를 적용 함!!!!  # predict()메서드의 매개변수 값으로 입력값 x가 2차원 배열로 전달 되었다고 가정하겠음 # -&gt; 매개변수 x에 이전에 만들어둔 x_train이 들어올 것인데 # x_train.shape = (455, 30) 즉 샘플 455개인데 각 샘플당 특성이 30개인 데이터 이므로 # 2차원 배열로 이루어 져 있을 것이다.[[샘플1], [샘플 2], [샘플 3], [] ~~~ ] (각 샘플당 30개의 특성) # ex) 샘플 1 = [1, 2, ~~~ , 30] (꼭 숫자가 1, 2, 3 ~~ 이 아니라 갯수를 나타낸 것임)    # 로지스틱 회귀 모델을 훈련시켜 보자  # 모델을 훈련해보자 neuron = LogisticNeuron() # LogisticNeuron에 대한 객체를 만듬 (객체를 모델이라고 부르기도 함) neuron.fit(x_train, y_train) # 이전에 만들어둔 훈련 세트로 (입력, 타깃 훈련 세트) 모델 훈련시키기   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: RuntimeWarning: overflow encountered in exp   # 테스트 세트 사용해서 모델의 정확도를 평가해 보자  np.mean(neuron.predict(x_test) == y_test)  # predict() 메서드의 반환값은 Ture 혹은 False가 원소인 (m, ) 크기의 배열 # y_test는 0 또는 1이 원소인 (m, ) 크기의 배열이기 때문에 비교 가능  # np.mean() 함수 : 매개변수 값으로 전달한 비교문 결과의 평균값을 계산하여 반환 -&gt; 이것을 정확도(accuracy)라고 한다.   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: RuntimeWarning: overflow encountered in exp      0.8245614035087719   지금까지 로지스틱 회귀에 대해 직접 구현해 보았다.   사실 지금 구현한 로지스틱 회귀 뉴런이 단일층 신경망이다.   구체적인 내용(층에 대한 내용 등등)은 이후에 설명해 볼 것이다.   또한 로지스틱 회귀를 구현하는데 생각보다 귀찮은데 이 부분 또한 사이킷런에서 제공하는 클래스를 이용하면 편리하다.   이 부분도 이후에 설명할 것이다.     Reference    박해선, 딥러닝 입문, 이지스퍼블리싱, 2019, 76~104pg   ","categories": ["Deep_Learning"],
        "tags": ["로지스틱 회귀","퍼셉트론","아달린","계단함수","활성화 함수","로지스틱 손실함수","미분 연쇄 법칙"],
        "url": "/deep_learning/Binary_Classification/",
        "teaser": null
      },{
        "title": "MySQL을 시작하면서..",
        "excerpt":"처음 MySQL을 배우려고 하는 이유는 백엔드든 AI든 데이터를 이용하고, 데이터를 전처리하고 가공할 줄 알아야 한다고 전해 들었기 때문이다.   많은 RDBMS 중 왜 하필 MySQL을 배우는지는 그냥 단순히 예전부터 많이 가장 많이 들어왔고 그렇기 때문에 가장 많이 쓰이지 않을까 해서이다..     MySQL을 배우기 이전에 MySQL이 뭔지?? 또 더 이전에 데이터베이스와 데이터베이스를 관리하는 DBMS에 대해 알아가면서 자연스럽게 MySQL에 대해 알아보려 한다.       데이터베이스(DB) :  데이터의 집합을 의미하며, 데이터의 저장공간 자체를 의미하기도 한다. 특히 MySQL에서는 데이터베이스를 자료가 저장되는 디스크 공간 (주로 파일로 구성되어 있는)으로 취급한다.     DBMS(DataBase Management System) :  데이터베이스를 관리, 운영하는 역할을 하는 소프트웨어의 개념   또한 여러명이 사용자나 응용 프로그램이 DBMS가 관리하는 데이터에 동시에 접속하고 데이터를 공유하게 되므로 DBMS에서 데이터가 집중적으로 관리되고 있을 것이다.   MySQL이 대표적인 DBMS 중 하나로   MySQL, MariaDB, Oracle, SQLite 등등 여러 회사의 DBMS 제품이 있다.     데이터베이스(DB) 와 DBMS의 특징   1. 데이터의 무결성 :  DB 안의 데이터는 어떤 경로를 통해 저장되었던 간에 데이터에 오류가 있어서는 안된다.   이러한 무결성의 특징을 지키기 위해 DB는 제약 조건(Constraint)이라는 특성을 가진다.     2. 데이터의 독립성 :  DB의 크기를 변경 혹은 데이터 파일의 저장소를 변경하더라도 기존에 작성된 응용프로그램에는 절대 영향을 끼쳐서는 안된다.   즉 서로 독립적인 관계여야 한다. (의존적 X)     3. 보안 :  DB안의 데이터에 아무나 접근할 수 있는 것이 아니라 데이ㅓ를 소유한 사람이거나 데이터에 접근 허가된 사람만 접근할 수 있어야 한다.   또한 사용자의 계정에 따라 다른 권한을 가짐!!     4. 데이터 중복의 최소화 :  동일한 데이터가 여러개 중복되어서 저장되면 안된다.     5. 응용 프로그램 제작 및 수정이 쉬움 :  기존에 존재하는 파일시스템을 사용할 때 각각 파일의 포매셍 맞춰 개발해야하는 응용 프로그램을 DB를 이용하면 통일된 방식으로 응용 프로그램 작성이 가능해지게 된다. 따라서 유지보수가 쉬워진다.   6. 데이터의 안정성 향상 :  대부분 DBMS가 제공하는 백업, 복원 기능을 제공하기 때문에 데이터에 문제가 생겼을 경우 복원 복구할 수 있다.       DBMS 유형 :  DBMS의 유형은 여러가지가 존재한다. 우리가 배울 MySQL은 어떤 한 유형의 DBMS에서 쓰인다.   1. 계층형 DBMS (Hierarchical DBMS) :  각 계층이 트리 형태를 가지며 1:N 관계를 가진다. ex)  노드 1   노드 1 밑에 노드 2-1 노드 2-2가 연결되어 잇고  노드 2-1 밑에 노드 3-1, 노드 3-2가 연결되어 밑에 있고 ~~~~       이 구조의 장점은   1) 주어진 상태에서 검색이 상당히 빠르다.      이러한 구조는 엄청난 단점이 존재한다.   1) 구조를 변경하기 어렵다.   2) 접근의 유연성이 떨어진다. -&gt; 따라서 임의의 검색에 어려움이 있다.     2. 망형 DBMS (Network DBMS) :  계층형 DBMS의 단점을 개선하기 위해 생겨난 구조로 1:1, 1:N, N:M 관계가 존재하여 효과적이고 빠른 데이터 추출이 가능해졌다.    ex)  계층형 DBMS 구조에서 같은 층의 노드끼리도 연결이 되어 있거나, 노드 3-2가 노드 2-1과 연결되어 있을 뿐 아니라 노드 2-2 와도 연결 될 수 있는 구조이다.   이 뿐만 아니라 이전에 설명한 1:1, 1:N, N:M 구조도 모두 가능      하지만 단점이 아직 존재하는데.   계층형 DBMS와 마찬가지로 매우 복잡한 내부 포인터를 사용하고 있어 프로그래머가 이 모든 구조를 이해해야만이 프로그램의 작성이 가능하다. (1:1, 1:N, N:M 등 여러 관계가 가능하다 보니 계층형 구조보다 더 복잡함)        3. 관계형 DBMS(Relational DBMS) RDBMS :  데이터베이스를 테이블(table)이라고 불리는 최소 단위로 구성   테이블은 하나 이상의 열로 구성 되어있음!!     RDBMS는 모든 데이터는 테이블에 저장되므로 테이블이라는 구조가 가장 기본적이고 중요한 구성이다!!       왜 테이블로 데이터를 저장하려 했을까?? -&gt; 데이터를 효율적으로 저장할 수 있는 구조이기 때문이다.   데이터를 저장할때 여러 테이블을 나누어 저장하여 불필요한 공간 낭비 줄일 수 있고, 데이터 저장의 효율성을 높이기 때문이다.     위의 장점 뿐 아니라 RDBMS는 다른 유형의 DBMS에 비해 변화에 순응할 수 있으며, 유지보수에도 편리한 특징을 가지고 있다.   또한 이전에 설명한 DB와 DBMS의 특징인 무결성을 잘 보장해 주기 때문에 동시에 데이터에 접근하는 응용프로그램 사용시 RDBMS를 사용하는 것이 좋을 가능성이 크다.      물론 장점만 있는것은 아니다..   단점 으로느 시스템 자원을 많이 차지해서 시스템 속도가 전반적으로 느려진다.     속도가 정말 중요한 사회에서 너무 큰 단점 아닌가??   다행이도 하드웨어의 발전으로 인해 이 단점을 매꾸어 줄 정도로 하드웨어 성능으로 속도를 많이 보완 하였다.      다른 유형은 DBMS와 다르게 RDBMS의 장점을 많이 나타냈고, 단점 또한 그렇게 심하게 표현하지 않은것으로 보아 우리가 배울 MySQL이 RDBMS 이다..      이제 DB, DBMS에 대해 어느정도 알겠는데.. 그런데 도대체 SQL이 뭐지 ?   SQL(Structed Query Language) :  Language에서 알 수 있듯 언어이다.   이전에 MySQL이 RDBMS 중 하나라고 설명하였다.   즉 SQL이 RDBMS에 쓰이는 언어임을 유추해 볼 수 있을 것이다.   다시 SQL은 관계형 데이터베이스에서 사용되는 언어이다.   즉 RDBMS를 배우기 위해 SQL을 알아야 한다.       여기서 헷갈리는 점이 SQL은 RDBMS에 쓰이는 언어이고   MySQL은 RDMBS 중 하나이면 MySQL은 SQL 중 하나가 아니네??   -&gt; 맞다.. MySQL은 RDBMS 제품 중 하나의 이름일 뿐이다..   MySQL은 RDBMS이므로 MySQL에서 SQL을 사용하겠네… 라고 이해 하면 된다.      SQL의 특징   1. DBMS 제작 회사와 독립적  이전에 RDBMS 제품이 MySQL, MariaDB, Oracle등 다양하다고 하였다. 그런데 RDBMS에서 사용된는 언어인 SQL이 여러 제작 회사에 따라 다르다면 어휴… 회사 제품(RDBMS)마다 적합한 SQL을 공부해야하니 생각만해도 끔찍하다..   다행이도 SQL은 표준 SQL이 존재하고 각 제작회사들은 이 표준 SQL에 맞춰서 RDBMS를 만든다. (물론 그.. 제품마다 다른 명령어(언어) 등이 존재하긴 한다.)     2. 다른 시스템으로 이식성이 좋음  SQL은 서버, 개인, 휴대용 장비에 운영되는 DBMS마다 상호 호환성이 뛰어나기 때문에 다른 시스템으로의 이식성이 좋다.     3. 표준이 계속 발전  SQL 표준이 계속적으로 개선되고 발전되고 있다.   ex) SQL-86, SQL-92, SQL:1999, SQL:2019 등등    4. 대화식 언어이다.  주피터 노트북으로 파이썬 언어를 작성해 보았으면 알 것이다.   기존 프로그래밍 언어처럼 코드 작성하고 컴파일, 디버깅 실행 과정 거쳐서 결과를 확인할 수 있는것과 달리 바로바로 코드 한줄 한줄 바로 결과를 확인할 수 있다는 장점이 있다.   SQL도 이러한 장점이 있는 바로 질의하고 결과를 얻는 대화식 언어이다.    5. 분산형 클라이언트 / 서버 구조  SQL은 분산형 구조인 클라이언트 / 서버 구조를 지원한다.   클라이언트 / 서버 구조란 클라이언트에서 질의를 하면 서버에서 질의를 받아서 처리하고 그에 대한 대답을 클라언트에게 전달하는 구조이다.      주의 !!!   이전에 설명하듯 표준 SQL이 존재하지만 제조사별로 각자의 RDBMS에서 SQL이 동일하지 않은 경우가 있다.       마지막으로 앞으로 배울 MySQL의 특징에 대해 알아보자.  MySQL :  Oracle사에서 제작한 RDBMS로 오픈 소스로 제공된다. (무료라는 말 ㅎㅎ) -&gt; 이게 장점 중 하나임    MySQL의 또다른 장점으로는 특히 대용량 데이터베이스 운영하기 위한 기술들이 많이 포함되어 있다는 것이다.  ","categories": ["MySQL"],
        "tags": ["SQL","DBMS","RDBMS"],
        "url": "/mysql/MySQL_start/",
        "teaser": null
      },{
        "title": "순수 자바로만 작성하는 예제_회원 도메인 개발",
        "excerpt":"Spring이 왜 생기게 되었는지 알기위해  순수한 자바코드로  예제를 작성해보며 Spring의 필요성을 느껴보고자 한다.       사용할 IDE : IntelliJ     프로젝트 생성 : start.spring.io에서  Project : Gradle Project   Spring Boot : 2.5.4  Language : Java  Packaging : Jar  &lt;/b&gt;Dependencies : 선택 X &lt;/b&gt;    아니 순수 자바코드로만 작성한다 해놓고 왜 스프링 부트 스타터로 프로젝트 생성 ??   -&gt; 단순히 프로젝트 환경설정을 편리하게 설정하기 위해서 임, Dependencies 아무것도 설정 안했으니 Spring boot가 core 쪽 library만을 가지고 project 만든다.      예제는 특정 상황을 가정할 것이다.   비즈니스 요구사항이 주어질 것이고 그 요구사항에 맞게 설계를 할 것이다.  처음에는 순수 자바코드로 작성할 것임 !!     요구 사항 :  회원 :     회원가입을하고 해당 회원을 조회할 수 있음    회원은 2가지 등급이 존재  (1) 일반 등급, (2) VIP 등급     회원 데이터는 자체 DB를 구축할 수도 있고 외부 시스템과 연동할 수도 있음 -&gt; 미확정인 상태임 !!!   주문과 할인 정책 :     회원가입을 한 회원은 상품을 주문할 수 있음    회원은 등급에 따라서 할인 정책을 적용받을 수 있음     할인 정책은 모든 VIP등급의 회원들에게 1000원을 할인해 주는 고정 금액 할인을 적용해 달라는 요구가 있음 (나중에 변경될 수 있음)     할인 정책은 변경가능성이 높기 때문에 (아직 기본 할인 정책을 회사에서 정하지 못함), 오픈 직전까지 이 고민을 미루려고 함    혹은 할인을 적용 안할수도 있음      ### 요구 사항에서 약간 골치아픈 문제가 2개이다.       회원 데이터에 대한 부분이 아직 미확정인 상태     할인정책이 변경가능성이 크고 적용 안할 가능성 또한 있음       이러한 문제점이 있다고 해서 정책이 확실히 결정되기 까지 해당 부분 개발을 하지않고 기다릴 수는 없다. &lt;br. 그럼 어떻게 해결 해야할까??      이전에 객체 지향 설계 방법의 장점을 기억할 것이다.   역할과 구현을 나누면 (인터페이스와 구현 객체 생각!!) -&gt; 유연하고 변경이 용이해짐   즉 인터페이스를 만들고 구현체를 얼마든지 갈아끼울 수 있도록 설계하면 됨        설계를 시작해보자   먼저 도메인을 설계 해보자.   회원 도메인 설계   회원 도메인 요구 사항     회원가입 할 수 있고 회원가입한 회원을 조회할 수 있음     회원은 일반과 VIP 두 가지 등급 존재     회원 데이터는 자체 DB를 구축 할 수 잇고 외부 시스템과 연동할 수도 있다. (미확정인 상태)     회원 도메인 협력 관계     여기서 클라이언트, 회원 서비스, 회원 저장소 3개는 역할   회원 데이터를 어떻게 저장하지 아직 확실히 정하지 않았으니   일단 먼저 DB를 구축하기 전에 기본적인 메모리 회원 저장소를 구현하고   차 후에 DB 회원 저장소, 외부 시스템 연동 회원 저장소를 구현 하려 한다.  즉 회원 저장소 역할의 구현으로 메모리 회원 저장소, DB 회원 저장소, 외부 시스템 회원 저장소 3가지를 구현하고자 한다.      회원 클래스 다이어그램     회원 도메인 협력 관게를 클래스로 나타내보면 회원 클래스 다이어그램과 같다.   각 역할들을 interface로, 역할들에 대한 구체적인 구현을 인터페이스에 상속받은 클래스로 구현하였다.     간단히 정리하자면   인터페이스 : 역할   상속받은 다른 클래스 : 구현     인터페이스는 메서드 선언만 하고   상속 받은 다른 클래스는 implements 상속으로 메서드 정의 (메서드 오버라이딩)     MemoryMemberRepository 클래스 경우 일단 간단한 메모리 저장하는 역할을 하는 클래스를 정의 하였고   DbMembeerRepository 클래스 경우 자체 DB 구축 에 대해서 정의할 클래스이다. (DB 부분은 차후에 정의할 것)       이렇게 역활과 구현을 분리해 두면 유연하고 변경이 용이해 지기 때문에 이전에 비즈니스 요구사항 중 회원 데이터 부분이 아직 미확정 이라고 하였는데 나중에 변경이 일어나도 유연하게 대처가 가능해진다.      회원 객체 다이어 그램       클래스 다이어그램에서 정의한 클래스를 이용해 객체를 만들어 객체간의 참조 구조를 작성해보면 위와 같다.      회원 서비스 경우에는 new MemberServiceImpl(); 을 통해    메모리 회원 저장소경우에는 new MemoryMemberRepository();를 통해    객체를 생성하고자 한다.       회원 도메인 구조를 기준으로 코드로 작성해보도록 하자.     코드 작성에 앞서 패키지 이름은 spring_basic이다.   코드 윗부분에 패키지, 클래스 명을 보면 파일 구조를 파악할 수 있을 것이다.     마지막 부분에 전체적인 파일 구조를 정리하도록 하겠다.     Gradle 전체설정  build.gradle  plugins { \tid 'org.springframework.boot' version '2.5.4' \tid 'io.spring.dependency-management' version '1.0.11.RELEASE' \tid 'java' }  group = 'hello' version = '0.0.1-SNAPSHOT' sourceCompatibility = '11'  repositories { \tmavenCentral() }  dependencies { \timplementation 'org.springframework.boot:spring-boot-starter' \ttestImplementation 'org.springframework.boot:spring-boot-starter-test' }  test { \tuseJUnitPlatform() }   회원 엔티티에 대해 작성해 보자     회원 등급   package hello.spring_basic.member;  public enum Grade { //Enum으로  회원의 등급 설정     Basic,     VIP }  //Enum class는 열거형이라 불리며 서로 연관된 상수들의 집합을 의미 //(기존 상수를 정의하던 final static string과 같이 문자열이나 숫자들을 나타낸는 기본자료형의 값을 enum 이용해서 나타낼 수 있음)   회원 엔티티   package hello.spring_basic.member;  public class Member { //회원 entity에 대한 클래스를 만듬     private Long id;     private String name;     private Grade grade;      public Member(Long id, String name, Grade grade) {         this.id = id;         this.name = name;         this.grade = grade;     }      public Long getId() {         return id;     }      public void setId(Long id) {         this.id = id;     }      public String getName() {         return name;     }      public void setName(String name) {         this.name = name;     }      public Grade getGrade() {         return grade;     }      public void setGrade(Grade grade) {         this.grade = grade;     } }    회윈 저장소에 대해 작성해 보자   회원 저장소 인터페이스  package hello.spring_basic.member;  public interface MemberRepository {  //인터페이스     void save (Member member); // 회원을 저장하는 메서드      Member findById(Long memberId); //회원의 아이디로 회원을 찾는 메서드 }   메모리 회원 저장소 구현체  package hello.spring_basic.member;  import java.util.HashMap; import java.util.Map;  public class MemoryMemberRepository implements MemberRepository {     //implements : 부모 객체는 선언만 하며 정의(내용)은 자식에서 오버라이딩 해서 사      private static Map&lt;Long, Member&gt; store = new HashMap&lt;&gt;(); // 데이터 저장하기 위해 해시맵에 데이터 저장      // HashMap은 사실 동시성 문제가 발생할 수 있으므로 그런 경우에는 ConcurrentHashMap 사용하면 된다.       @Override     public void save(Member member) {         store.put(member.getId(), member);     } //회원 저장하는 메서드 정의      @Override     public Member findById(Long memberId) {         return store.get(memberId);     } //회원 ID로 회원 찾는 메서드 정의          //인터페이스에서 선언한 메서드를 구체적으로 메서드 정의 -&gt; 구현 }     DB가 아직 확정이 되지 않았음. 하지만 그렇다고 개발을 안할수는 없음.   -&gt; 개발은 진행하되 단순한 메모리 회원 저장소(단순히 해시맵에 저장하여 구현)를 구현해서 개발 진행할 것      회원 서비스에 대해 작성해 보자.   회원 서비스 인터페이스   package hello.spring_basic.member;  public interface MemberService {      void join (Member member); //회원 가입 메서드 선언      Member findMember(Long memberId); // 회원 조회 메서드 선언 }    회원 서비스 구현체   package hello.spring_basic.member;  public class MemberServiceImpl implements MemberService {      private final MemberRepository memberRepository = new MemoryMemberRepository(); // 회원 가입 메서드 정의      @Override     public void join(Member member) {         memberRepository.save(member);         /*         save메서드 호출시 다형성에 의해 인터페이스인 MemberRepository클래스가 아닌 MemoryMemberRepository 클래스에 있는 save함수를 호출한다.         */     }      @Override     public Member findMember(Long memberId) {         return memberRepository.findById(memberId);     } // 회원 조회 메서드 정의          // 인터페이스에서 선언한 메서드를 구체적으로 메서드 정의 -&gt; 구현 }        자 이제 회원 도메인 개발을 해 보았으니 제대로 작동하는지 테스트를 해보자.       회원 도메인 실행과 테스트를 해보자.   회원 도메인 - 회원 가입 main  main 함수를 만들어 직접 회원을 만들어 회원가입 시키고 회원가입이 되었는지 확인해보자.     package hello.spring_basic;  import hello.spring_basic.member.Grade; import hello.spring_basic.member.Member; import hello.spring_basic.member.MemberService; import hello.spring_basic.member.MemberServiceImpl;  public class MemberApp {      public static void main(String[] args) {         MemberService memberService = new MemberServiceImpl(); // MemberService의 객체 memberservice 생성 (MemberServiceImpl 구현 가지는)         Member member = new Member(1L, \"memberA\", Grade.VIP);// 새로운 Member의 객체 member 생성         //이름은 memberA, 등급은 VIP          memberService.join(member); // 새로운 member를 등록 (회원가입)          Member findMember = memberService.findMember(1L);//위 사람이 제대로 등록(회원 가입) 되었는지 확인해보자.          System.out.println(\"new member = \" + member.getName());         System.out.println(\"find Member = \" + findMember.getName()); //회원가입이 잘 되었다면 member.getName()과 findMember.getName()이 같은 출력을 내놓아야야          //똑같은 \"memberA\"를 출력함     } }    main 함수를 실행시켜보면        위 그림과 같이 문제없이 잘 실행이 된다.      하지만 이러한 방법으로 테스트 하는 것은 좋지 않은 방법이다.   (애플리케이션 로직으로 위의 방식으로 테스트 하는 방법)      -&gt; JUnit 테스트를 이용한다  JUnit은 자바 프로그래밍 언어용 유닛 테스트 프레임워크로   @Test 메서드가 호출되면 독립적인 테스트가 가능하다. -&gt; 어노테이션으로 편리하게 테스트 가능        회원 도메인 - 회원 가입 테스트   package hello.spring_basic.member;  import org.assertj.core.api.Assertions; import org.junit.jupiter.api.Test;  public class MemberServiceTest {      MemberService memberService = new MemberServiceImpl();      @Test     void join() {         //given : 이런 이런 환경 주어졌을때         Member member = new Member(1L, \"memberA\", Grade.VIP);          //when : 이렇게 했을때         memberService.join(member);         Member findMember = memberService.findMember(1L);          //then : 이렇게 된다. -&gt; 검증         Assertions.assertThat(member).isEqualTo(findMember);          //정리하면 새로운 member가 주어졌을때         // 그 새로운 멤버를 회원가입(등록) 했을때         // member와 findMember가 같아야 한다.     } }   MemberServiceTest 클래스를 실행시켜보면     위 그림에서 아무 문제없다는 결과를 내 놓는다.       지금까지 회원 도메인을 설계하여 실행해보고 테스트 또한 해보았는데 아무 문제없이 실행되었던 것 같다.   실행이 잘 되었다고 설계에 있어서 문제점은 없었을까?     회원 도메인 설계의 문제점 :  MemberServiceImpl 클래스를 보면     public class MemberServiceImpl implements MemberService {      private final MemberRepository memberRepository = new MemoryMemberRepository();       ~~~  위 코드를 보면   memberRepository는 interface인 MemebrRepository에 의존하고 있다.   그런데 memberRepository = new MemoryMemberRepository(); 이 코드를 보면   실제 할당하는 부분이 구현체에 의존하고 있음을 알 수 있다. (MemoryMemberRepository는 구현체)     따라서 MemberServiceImpl 클래스는 추상화와 구체화 모두에 의존하고 있다.   -&gt; DIP를 위반하고 있다는 의미이다. 즉 OCP 원칙을 지키지 못한 상황이다.   -&gt; 이 문제를 잘 기억하고 있다가 나중에 어떻게 해결하는지 알아보도록 하자.       Reference :  김영한 강사님 ㅅ프링 핵심 원리 - 기본편  강의 중  ","categories": ["Spring"],
        "tags": ["순수 자바 코드로 개발","회원 도메인","실행과 테스트"],
        "url": "/spring/spring_basic(2)/",
        "teaser": null
      },{
        "title": "단일층 신경망과 사이킷런으로 로지스틱 회귀 수행",
        "excerpt":"이전에 로지스틱 회귀를 직접 구현해 보았다.   로지스틱 회귀이 단일 신경층 망(single layer neural network) 동일하다고 한다.   신경망??? 단일층 ??? 이부분에 대해 좀더 자세히 알아보고,   또한 이 단일 신경층 망 즉 로지스틱 회귀를 구현하는 것이 귀찮기 때문에 사이킷 런으로 간편하게 로지스틱 회귀를 수행해 보자       신경망(Neural Network) :       위 그림을 보면 일반적인 신경망의 구조로 가장 왼쪽에 입력층(input layer), 가장 오른쪽이 출력층(output layer), 그리고 가운데 층들을 은닉층(hidden layer)라 부른다. 또한 작은 원으로 표시된 활성화 함수는 은닉층과 출력층의 한부분에 속한다.      그럼면 이전에 로지스틱 회귀는 단일층 신경망과 같다고 하였는데 단일층 신경망에 대해 알아보자.     단일층 신경망(Single Layer Neural Network) :  단일층 신경망은 일반적인 신경망의 구조에서 은닉층이 없는 신경망 즉 입력층과 출력층만 가지는 신경망을 의미한다.   로지스틱 회귀는 이러한 특징을 가지는 단일층 신경망이다.          지금부터 단일층 신경망을 구현해보고자 한다.   로지스틱 회귀가 단일층 신경망이라며 그러면 이전에 구현한 로지스틱 회귀가 단일층 신경망 구현한것 아님 ???   맞는데 다시 구현하려는 이유가 몇가지 유용한 기능 추가하기 위해서이다.  대표적으로 선형 회귀나 로지스틱 회귀 모두 경사 하강법을 사용했는데 이 방법은 손실함수의 결괏값을 최소화 하는 방향으로 가중치, 절편을 업데이트 하엿다.   선형 회귀 경우 손실 함수로 제곱 오차 손실 함수를,   로지스틱 회귀 경우 손실 함수로 로지스틱 손실 함수를 사용했었다.     만약 손실 함수의 결괏값이 줄어들지 않는다면 어떠한 문제가 있다는 것인데 그 결괏값이 줄어들고 있는지 확인하기 위한 기능을 추가할 것이다.   그럼 코드로 구현해 보자.   # 손실 함수의 결괏값 저장하는 기능을 추가하자  # 먼저 이전에 구현한 로지스틱 회귀 LogisticNeuron 클래스를 그대로 복사 후 클래스 이름을 SingleLayer로 바꾸자 # 여기에 수정 및 새로운 기능을 추가할 것이다.  # 기존 LogisticNeuron 클래스의 # __init__() 메서드에 self.losses 리스트를 만들어서 손실 함수의 결괏값을 저장할 것임  # 각 샘플마다 손실 함수를 계산하고 그 결괏값을 모두 더한 후 샘플 개수로 나눈 평균값을 구할 것임 # 그 평균값을 self.losses에 저장할 것임 # 그리고 self.activation() 메서드로 계산한 a를 np.log()계산을 위해 np.clip()을 적용할 것이다. # 이게 무슨말 ?? # a가 만약 0에 가까워지게 되면 np.log() 값은 음의 무한대로 가게 되고 a가 1에 가까워 지면 np.log() 값은 # 0에 수렴한다. 따라서 손실값이 무한해지면 정확한 계산히 힘들어지기 때문에 a의 값의 범위를 조절하고 싶다. # 이때 np.clip()을 이용하면 된다. # 여기에선 np.clip(a, 1e - 10, 1 - 1e - 10)을 적용하여 a의 값이 # -1 x 10^(-10) ~ 1 - 1 x 10^(10) 사이가 되도록 조절한다. # np.clip()은 주어진 범위 밖의 값을 잘라 내어 이러한 기능을 구현한다.   import numpy as np from math import *  class SingleLayer:      def __init__(self):     self.w = None     self.b = None      self.losses = [] # 손실 함수의 결괏값을 저장할 리스트    def forpass(self, x):      z = np.sum(x * self.w) + self.b     return z    def backprop(self, x, err):      w_grad = x * err     b_grad = 1 * err       return w_grad, b_grad      def activation(self, z):     z = np.clip(z, -100, None) # 안전한 np.exp() 계산을 위해 클리핑한다.     a = 1 / (1 + np.exp(-z))      return a    def fit(self, x, y, epochs = 100):     self.w = np.ones(x.shape[1])      self.b = 0      for i in range (epochs):       loss = 0       indexes = np.random.permutation(np.arange(len(x))) # indexes를 섞음        # ** np.random.permutation()을 써서 왜 indexes를 굳이 섞지??? 이부분에 대해서는 아래에 설명하겠다.        for i in indexes:         z = self.forpass(x[i])         a = self.activation(z)          err = -(y[i] - a)          w_grad, b_grad = self.backprop(x[i], err)           self.w -= w_grad           self.b -= b_grad           a = np.clip(a, 1e-10, 1 - 1e-10) # 안전한 로그 계산을 위해 클리핑한 후 손실을 누적함          loss += -(y[i] * np.log(a) + (1 - y[i]) * np.log(1 - a)) # 에포크 마다 평균 손실을 구함              self.losses.append(loss/len(y)) # 구한 손실을 losses에 저장    def predict(self, x):     z = [self.forpass(x_i) for x_i in x]     return np.array(z) &gt; 0 # 계단 함수 적용      def score(self, x, y):     return np.mean(self.predict(x) == y)  # 위에 score() 메서드를 추가하고 predict() 메서드를 기존의 LogisticNeuron 클래스에서 수정을 하였다.  # 시그모이드 함수의 출력값이 0 ~ 1 사이 이므로 이것을 확률값으로 보고 # 양성 클래스를 판단하는 기준이 0.5 이상이다. # z가 0보다 크게 되면 시그모이드 함수의 출력값이 0.5보다 커지고 # z가 0보다 작으면 시그모이드 함수의 출력값은 0.5보다 작다.  # 따라서 predict() 메서드에 굳이 시그모이드 함수를 사용하지 않고 z값이 0보다 큰지 작은지만 판단하면 된다.  # 그래서 predict() 메서드를 굳이 로지스틱 함수 적용 안하고 z 값 크기로 비교하여 결과 반환함.   위 SingleLayer 클래스를 구현할 때 왜 np.random.permutation()을 사용하여 indexes를 굳이 섞는지 궁금해 햐였다   이 부분을 설명하기 위해서 여러가지 경사하강법에 대해 먼저 알아 보아야 한다.      여러가지 경사 하강법 :  지금까지 사용해왔던 경사 하강법을 생각해보자..       def fit(self, x, y, epochs = 100):      self.w = np.ones(x.shape[1]) #가중치를 초기화 한다. np.ones()함수를 이용해 1로 초기화      self.b = 0 #절편을 0으로 초기화       for i in range (epochs): # epochs만큼 반복        for x_i, y_i in zip(x, y):          z = self.forpass(x_i) # 선형함수식에 대입하여 z 구함          a = self.activation(z) # z를 활성화 함수에 대입하여 a 구함          err = -(y_i - a) # 오차를 계산함          w_grad, b_grad = self.backprop(x_i, err) # 역방향 계산으로 가중치 gradient와 절편 gradient 구함          self.w -= w_grad  # 가중치를 갱신한다.         self.b -= b_grad # 절편을 갱신한다.  위 코드는 LogisticNeuron 클래스의 fit() 메서드 이다.  경사하강법을 통해 가중치와 절편을 갱신하는데 x, y 샘플에서 z와 a를 갱신할때 x_i, y_i 이렇게 하나의 샘플만 뽑아서 하나의 샘플 데이터 1개에 대해서만 gradient를 구하여 갱신하고 있다.   즉 전체 샘플 수 만큼 만복해서 계산하지만 1개의 샘플 데이터를 뽑아서 gradient 구함   확률적 경사 하강법(Stochastic Gradient Descent) :  위의 fit()메서드 즉 지금까지 사용해왔던 경사 하강법인 샘플 데이터 1개에 대한 gradient 계산으로 가중치와 절편을 갱신을 하고. 이러한 경사 하강법을 확률적 경사하강법 이라고 한다.   사실 엄밀한 확률적 경사 하강법은 아니다. 왜냐하면 엄밀한 확률적 경사하강법은 gradient 계산을 위해 샘플 데이터 1개를 뽑을때 위의 방법처럼 순서대로 1개씩 뽑는게 아니라 무작위로(랜덤으로) 뽑아야 한다. -&gt; 확률적    즉 np.random.permutaiton()을 사용하는 이유가 무작위로 샘플을 1개씩 뽑기 위함이다.   -&gt; 엄밀한 확률적 경사하강법 적용 위해   이 부분은 뒤에서 구체적으로 설명할 것이다.        1개 샘플을 중복되지 않게 무작위로 선택 후 gradient를 계산한다.     배치 경사 하강법 (Batch Gradient Descent) :  전체 훈련 세트를 사용하여 한 번에 gradient를 계산하는 방식      전체 샘플을 모두 선택하여 gradient 계산을 한다 (epoch 마다)    미니 배치 경사 하강법(Mini-Batch Gradient Descent) :  배치의 크기를 작게 하여 (훈련 세트를 여러번 나눔) 처리하는 방식      전체 샘플 중 몇개의 샘플을 중복되지 않도록 무작위로 선택하여 gradient를 계산함.    여러가지 경사 하강법들의 장단점에 대해 알아보자      확률적 경사 하강법 :  장점 : 샘플 데이터 1개마다 gradient 계산하므로 계산 비용이 적게 든다.  단점 : 가중치, 절편이 최적값에 수렴하는 과정이 불안함.     images_reference : https://valuefactory.tistory.com/460       배치 경사 하강법 :  장점 : 전체 훈련 데이터 세트를 사용하여 한 번에 gradient 계산하므로 가중치, 절편이 최적값에 수렴하는 과정이 안정적임  단점 : 계산비용이 많이듬     images_reference : https://valuefactory.tistory.com/460       미니 배치 경사 하강법 :  확률적 경사 하강법과 배치 경사 하강법을 절충해서 만든 경사 하강법임        images_reference : https://valuefactory.tistory.com/460       자 이제 SingleLayer 클래스를 구현할 때 왜 np.random.permutation()을 사용하여 indexes를 굳이 왜 섞는지에 대한 의문으로 다시 돌아오자     -&gt; 엄격한 확률적 경사 하강법 적용 (샘플을 무작위로(랜덤하게) 뽑아서 gradient 계산)   매 epoch마다 후련 세트의 샘플 순서를 섞어 사용하자!!  위에서 살펴본 모든 경사하강법은 매 epoch마다 훈련 세트의 샘플 순서를 섞어서 가중치, 절편의 최적값을 계산해야 한다.   왜 ??   훈련 세트의 샘플 순서를 섞으면 최적의 값을 찾는 과정이 다양해지기 떄문에 손실 함수의 값이 줄어들고 그로 인하여 최적의 값을 제대로 찾을 수 있다.      위 그림을 보면 1번쨰 epoch에서 사용된 훈련 세트의 샘플 순서는 1, 2, 4   2번째 사용된 훈련 세트의 샘플 순서는 3, 1, 2이다.     이런식으로 훈련 세트의 샘플 순서를 섞는 방법은 넘파이 배열의 인덱스를 섞고 인덱스 순서대로 샘플을 뽑는 것이다.   이러한 방법이 훈련 세트 자체를 섞는 것보다 효울적이고 빠르다. (훈련 세트를 나두면 되니까)   이 방법은 np.random.permutation()함수를 사용하여 index를 섞는다.       즉 이전 까지는 그냥 샘플 순서대로 1개씩 뽑아서 gradient 계산하여 가중치와 절편을 갱신하였다.   하지만 이 방법이 가중치와 절편의 최적값을 제대로 찾을 수 었었다.   따라서 랜덤하게 샘플을 1개씩 뽑아서 gradient를 계산하면 최적의 가중치와 절편을 찾는 탐색과정이 다양해 지므로 더 좋은 결과를 내놓기때문에 이 방법을 사용할 것이다. (엄밀한 확률적 경사 하강법)     단일층 신경망 클래스 SingleLayer 클래스가 완성 되었으니 이전에 사용했던 유방암 데이터 세트에 적용해보도록 하자   # 단일층 신경망 훈련하기  # 단일층 신경망을 훈련하고 정확도를 출력 해볼 것이다. # 이전 LogisticRegression 클래스에서 하엿듯이 SingleLayer 객체를 만들고 훈련세트 x_train, y_train로 신경망 훈련하자 # 이후 score() 메서드로 정확도 출력해보자  from sklearn.datasets import load_breast_cancer cancer = load_breast_cancer()  x = cancer.data y = cancer.target  from sklearn.model_selection import train_test_split  x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.2, random_state = 42)  layer = SingleLayer() # 단일층 신경망 객체(모델) 생성 layer.fit(x_train, y_train) # 모델 훈련 layer.score(x_test, y_test) # 정확도 측정    0.9035087719298246   이전 LogisticRegression 클래스보다 정확도가 훨씬 높아졌다.  그 이유는 경사하강법에서 샘플 데이터를 순서대로 뽑아서 gradient 계산이 아닌 무작위 순서로 뽑아서 계산 -&gt; 손실함수의 값 줄임  즉 엄격한 확률 경사 하강법 적용을 하니 정확도가 높아졌음을 알 수 있다.   # 진짜 손실 함수의 누적값이 작아졌는지 한번 확인해 보자  import matplotlib.pyplot as plt  plt.plot(layer.losses) # 에포크당 손실 함수 누적값을 담아놓은 리스트인 lyaer.losses를 그래프로 그려보자 plt.xlabel('epoch') plt.ylabel('loss') plt.show()  #그래프 그려보니까 로지스틱 손실 함수의 값이 에포크가 진행할수록 감소한다. (물론 어느 횟수 이상부터는 크게 감소하지 않고 왔다갔다함)      정리해보면 로지스틱 회귀 알고리즘을 확장하면 가장 기초적인 신경망 알고리즘으로 보면 된다.  (이전에 구현한 LogisticRegression 클래스에서 좀더 수정, 추가하여 SingleLayerRegression 클래스 만듬)   SingleLayerRegression 클래스를 기초적인 단일층 신경망 알고리즘으로 보면 된다.     단일층 신경망을 구현하기 위해선 로지스틱 회귀 알고리즘을 구현해야 한다.   그런데 단일층 신경망이 필요할때 이것을 직접 구현하기란 정말 귀찮은 일이다.   이러한 귀찮은 일을 사이킷런에서 해결해준다.  사이킷런에서 이런 알고리즘을 미리 구현해 놓았기 때문이다.      SGDclassifier :  SGDClassifier 클래스는 사이킷런에서 제공하는 경사 하강법이 구현된 클래스이다.  그럼 SGDClassifier 클래스를 이용해 로지스틱 회귀 문제를 간단히 해결해 볼것이다   (SGDClassifier 클래스를 이용하면 로지스틱 회귀 문제 뿐만 아니라 여러 가지 문제에 경사 하강법 적용할 수 있다.)      # 사이킷런으로 경사 하강법을 적용해보자  # 로지스틱 손실 함수를 먼저 지정해보자  from sklearn.linear_model import SGDClassifier sgd = SGDClassifier(loss = 'log', max_iter = 100, tol = 1e-3, random_state = 42)  # SGDClassifier 클래스에 로지스틱 회귀 적용 위해 loss 매개변수에 손실 함수로 log를 지정 # max_iter는 최대 반복 횟수를 의미한다. # 만약 tol에 지정한 값만큼 로지스틱 손실 함수의 값이 감소되지 않으면 반복 중단한다. # (먄약 tol값 지정 안할시 max_iter 늘리라는 경고가 발생함) # random_state = 42로 지정하여 난수 초깃값을 42로 설정하여 반복 실행시 동일한 난수값 나오게 한다.    # 사이킷런으로 훈련하고 평가해보자  # SGDClassifier 클래스에는 이전에 직접 구현한 모델을 훈련시키는 fit() 메서드와 정확도를 측정하는 score() 메서드가 준비되어 있다. sgd.fit(x_train, y_train) sgd.score(x_test, y_test)   0.8333333333333334   # 마지막으로 사이킷런으로 특정 결과를 예측해보자  # SGDClassifier 클래스에는 이전에 직접 구현한 예측을 하는 predict() 메서드 또한 준비가 되어있다.  # !! 단 주의할 점은 사이킷런은 입력 데이터로 2차원 배열만 받아들인다.!!!!! # 만약 샘플 하나만 넣어야하는 경우에도 2차원 배열로 만들어서 넣어야 한다는 의미!!  # 배열 슬라이싱 이용해서 테스트 세트에서 10개의 샘플만 뽑아 예측해봄 sgd.predict(x_test[0:10])    array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0])   자 사이킷런에서 제공하는 클래스로 로지스틱 회귀를 직접 구현하지 않고도 이렇게 간단히 사용할 수 있다.  앞으로 사이킷런을 이용해서 이러한 방법으로 빨리 사용하는 경우가 대부분일 것이다.   그렇다고 지금까지 직접 구현해본 것이 아무 의미 없었던 일이 아니다.  내가 사용하고자 하는 어떠한 것이 어떠한 원리로 동작하는지를 안다면 더욱 그것을 적합한 상황에 잘 사용할 것이라 생각하기 때문이다.      Reference    박해선, 딥러닝 입문, 이지스퍼블리싱, 2019, 105~115pg  ","categories": ["Deep_Learning"],
        "tags": ["신경망","단일층 신경망","확률적 경사 하강법","배치 경사 하강법","미니 배치 경사 하강법","사이킷런","SGDclassifier"],
        "url": "/deep_learning/Single_Layer_Neural_Network/",
        "teaser": null
      }]
